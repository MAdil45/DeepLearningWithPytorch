{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4532b946-e26c-4173-96ca-5f0d4c5de280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.adil\\Miniconda3\\envs\\sahi\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9659dc69-7c09-47cb-882c-78236bcb171f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors: Multidimensional arrays\n",
    "# From Python lists to PyTorch tensors\n",
    "# Let’s see list indexing in action so we can compare it to tensor indexing. Take a list\n",
    "# of three numbers in Python\n",
    "a = [1.0, 2.0, 3.0]\n",
    "# We can access the first element of the list using the corresponding zero-based index:\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c943db-1b21-4cbb-9abd-2640d7f6784b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s construct our first PyTorch tensor and see what it looks like. It won’t be a particularly meaningful tensor for now, just three ones in a column:\n",
    "a = torch.ones(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b5881c-1dc7-4990-a646-187284a7b43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e548b6b3-bb8e-4b8e-983c-7724320e47dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "703cf28f-8f83-4556-9f9e-d018ffab0ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 3.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] = 3.0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1235b9e9-c56f-47c4-8f49-c1389184cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although on the surface this example doesn’t differ\n",
    "# much from a list of number objects, under the hood things are completely different.\n",
    "# Python lists or tuples of numbers are collections of Python objects that are individually\n",
    "# allocated in memory. PyTorch tensors or NumPy\n",
    "# arrays, on the other hand, are views over (typically) contiguous memory blocks containing unboxed C numeric types rather than Python objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a604283d-419c-4eee-ac05-bec0d4ca0efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say we have a list of coordinates we’d like to use to represent a geometrical object: perhaps a 2D triangle with vertices at coordinates (4, 1), (5, 3), and (2, 1). \n",
    "# Instead of having coordinates as numbers in a Python list, as we did earlier, we can use a one-dimensional tensor by storing Xs in the even indices \n",
    "# and Ys in the odd indices,\n",
    "# like this:\n",
    "points = torch.zeros(6)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0d86ed-6d17-44a8-878e-adfcc879771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points[0], points[1],points[2], points[3], points[4], points[5] = [4, 1, 5, 3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6823fa-cba2-41ae-8199-fed1d26ba67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1., 5., 3., 2., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df30f8b-c9e2-4df9-8b35-876d7d60240f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 1, 5, 3, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also pass a Python list to the constructor, to the same effect:\n",
    "points = torch.tensor([4, 1, 5, 3, 2, 1])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd901d1-e466-4024-94d2-dd737a4025dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the coordinates of the first point, we do the following:\n",
    "float(points[0]), float(points[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17664f73-fe6b-4a3e-895c-3fc511fe9f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is OK, although it would be practical to have the first index refer to individual 2D\n",
    "# points rather than point coordinates. For this, we can use a 2D tensor:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c3cbe1-5641-4289-9d5d-bdc9397efea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6cb220c-80f7-4629-bc42-0929812b9179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, we pass a list of lists to the constructor. We can ask the tensor about its shape:\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72f8ec2f-80b6-438b-a23f-1437a0afb38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4.), tensor(3.), tensor(2.))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can access an individual element in the tensor using two indices:\n",
    "points[0, 0], points[1,1], points[2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6426e73-8e36-4bf1-ae49-51c749da5314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing tensors\n",
    "# What if we need to obtain a tensor containing all points but the first? That’s easy using\n",
    "# range indexing notation, which also applies to standard Python lists. Here’s a\n",
    "# reminder\n",
    "somelist = list(range(6))\n",
    "somelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c68295d1-0c92-41e8-b665-4988eec76375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "somelist[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961bd1e9-92a0-4729-ac4b-f1bfb2c2f2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "somelist[1:4:2]   # From element 1 inclusive to element 4 exclusive, in steps of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0bbe92-d9b4-461e-a19f-ad7d4bf7d95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can use the same notation for PyTorch tensors, with the added\n",
    "# benefit that, just as in NumPy and other Python scientific libraries, we can use range\n",
    "# indexing for each of the tensor’s dimensions:\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0781a5ce-2c39-46c2-8d7a-9d8d4acbceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1:, 0] # all rows after the first, first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44e38dfa-b818-415f-8d26-2580f5e8890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[None]  # add a dimension of size 1 - just like unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23594cf9-dd7a-41f9-852f-9e89b47ddd64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3548,  1.1896, -1.1210, -0.5517,  0.9556],\n",
       "         [ 1.0760, -1.6869,  0.2269,  0.4788,  0.4645],\n",
       "         [ 0.8012,  0.5099,  0.5212,  0.4574,  0.1634],\n",
       "         [ 0.5559,  0.1012,  0.0905,  1.7435,  2.0226],\n",
       "         [-1.0296, -0.2199, -0.4711, -0.2198,  2.6266]],\n",
       "\n",
       "        [[-0.7974, -1.3237,  1.6206,  0.1641,  0.9851],\n",
       "         [ 0.0720,  0.1864,  1.6281,  0.5755,  1.5257],\n",
       "         [ 0.3105, -0.4134,  0.5031,  0.5214, -0.6756],\n",
       "         [ 1.7749,  2.1326, -0.2099, -1.2695,  0.8215],\n",
       "         [-0.7103, -2.3042, -1.1936,  2.2267, -0.6394]],\n",
       "\n",
       "        [[-0.4040, -0.1682, -0.1198, -1.8118,  2.7900],\n",
       "         [ 1.1330, -0.5533,  0.7714, -1.3545, -0.5088],\n",
       "         [ 2.4578,  1.5919, -0.5967,  0.6212,  0.2458],\n",
       "         [-0.0823,  0.1477, -1.1103, -0.1843, -0.4339],\n",
       "         [-1.8364,  0.4226,  0.2673,  0.0290, -0.6807]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Named tensors\n",
    "# The dimensions (or axes) of our tensors usually index something like pixel locations\n",
    "# or color channels. This means when we want to index into a tensor, we need to\n",
    "# remember the ordering of the dimensions and write our indexing accordingly\n",
    "# imagine that we have a 3D tensor like img_t from section\n",
    "# 2.1.4 (we will use dummy data for simplicity here), and we want to convert it to grayscale\n",
    "img_t = torch.randn(3, 5, 5)   # shape [channels, rows, columns]\n",
    "img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20c3ea1a-9110-43c2-ad05-760f4c73374a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.0356, -1.0156, -1.6433, -0.2836,  1.7230],\n",
       "          [ 1.2525,  0.4818, -0.4201, -0.1181, -0.2859],\n",
       "          [ 1.8440,  1.8716, -1.3844, -1.1459, -0.7177],\n",
       "          [-1.1183,  0.0586, -0.5382, -0.7843, -0.2889],\n",
       "          [-0.5446,  0.1936,  0.7380,  1.1355,  0.8134]],\n",
       "\n",
       "         [[ 0.3302,  0.1876,  0.0488,  1.1624, -1.0594],\n",
       "          [ 0.1395, -0.0300, -0.6096,  0.5992, -1.7151],\n",
       "          [ 0.1968, -0.4802,  0.8737,  1.0496, -1.1073],\n",
       "          [ 0.4446,  0.4609, -0.5842, -2.2817,  0.0064],\n",
       "          [-2.4679, -1.0416, -0.6903, -0.0493,  0.7675]],\n",
       "\n",
       "         [[-0.2497, -0.1667, -2.2239, -0.5858, -0.1180],\n",
       "          [ 0.1467, -1.1797, -0.6866, -0.4683,  0.2237],\n",
       "          [-1.3110,  0.0627, -0.7885,  1.3973,  0.5915],\n",
       "          [ 0.5110,  2.8850,  1.3243, -1.1376, -0.9300],\n",
       "          [ 0.8158, -1.9224, -0.2683, -0.8228,  0.0860]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2045, -0.7154,  0.2302, -1.2521, -1.0561],\n",
       "          [ 0.1737,  0.6896,  1.1324,  0.8446,  2.5281],\n",
       "          [-1.0914,  0.5151, -1.4923,  0.3030,  0.4524],\n",
       "          [-1.6463,  0.3949, -0.5548,  0.7100, -0.6565],\n",
       "          [ 0.6881, -0.7433, -0.1616, -0.9865,  1.3560]],\n",
       "\n",
       "         [[-0.6971, -0.6815, -0.0862, -2.0005, -0.5500],\n",
       "          [-0.1926,  0.4800, -0.6846,  0.2734,  1.7986],\n",
       "          [ 0.1869,  0.1004, -0.0581,  1.1743, -0.8465],\n",
       "          [-0.3366,  0.1092, -0.4109,  0.8019, -1.5305],\n",
       "          [ 0.9864, -1.1821, -0.7901, -1.4106,  0.9566]],\n",
       "\n",
       "         [[ 0.1720, -0.6682,  0.7038,  1.7596, -1.2958],\n",
       "          [ 0.1224,  1.2648, -2.5356,  0.5386, -1.2209],\n",
       "          [-0.7448, -0.6006, -1.5973, -0.2949,  1.3358],\n",
       "          [-1.6103,  0.1787, -1.5729, -0.6622, -0.6092],\n",
       "          [ 0.2765, -0.4504, -0.9318, -0.5542,  0.2975]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5) # shape [batch, channels, rows, columns]\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd066183-2280-4eb9-bd0c-1b7084396e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So sometimes the RGB channels are in dimension 0, and sometimes they are in dimension 1. But we can generalize by counting from the end:\n",
    "# they are always in dimension –3, the third from the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d46ea49b-d006-4f4a-9766-6bca5cc9aac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The lazy, unweighted mean can thus be written as follows:\n",
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "557fde6d-7a7a-421d-a0d7-920e222928ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed2f061d-8b17-44a8-95c5-cfd4b4a5f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsqueeze_weights = weights.unsqueeze(-1).unsqueeze_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "876e5b38-7803-4508-98a4-20e279a275e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_weights = (img_t * unsqueeze_weights)\n",
    "batch_weights = (batch_t * unsqueeze_weights)\n",
    "img_gray_weighted = img_weights.sum(-3)\n",
    "batch_gray_weighted = batch_weights.sum(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "124ab47b-8576-46dc-b844-cad032f8e686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 5, 5]), torch.Size([2, 3, 5, 5]), torch.Size([3, 1, 1]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights.shape, batch_t.shape, unsqueeze_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47081263-8e95-49b4-a7ef-ba5e0d121d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because this gets messy quickly—and for the sake of efficiency—the PyTorch function\n",
    "# einsum (adapted from NumPy) specifies an indexing mini-language2 giving index\n",
    "# names to dimensions for sums of such products. As often in Python, broadcasting—a\n",
    "# form of summarizing unnamed things—is done using three dots '…'; but don’t worry\n",
    "# too much about einsum, because we will not use it in the following\n",
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "batch_gray_weighted_fancy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aef80fc5-4cd8-4270-ac86-ae1108f8f48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.adil\\AppData\\Local\\Temp\\ipykernel_3704\\3497324698.py:6: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10/core/TensorImpl.h:1408.)\n",
      "  weighted_name = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is error-prone, especially when the locations where tensors are created and used are far apart in our code.\n",
    "# PyTorch 1.3 added named tensors as an experimental feature (see https://pytorch\n",
    "# .org/tutorials/intermediate/named_tensor_tutorial.html and https://pytorch.org/\n",
    "# docs/stable/named_tensor.html). Tensor factory functions such as tensor and rand\n",
    "# take a names argument. The names should be a sequence of strings:\n",
    "weighted_name = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels'])\n",
    "weighted_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd729589-8870-452e-b385-5d939d1a780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named:  torch.Size([3, 5, 5]) ('channels', 'rows', 'columns')\n",
      "batch named:  torch.Size([2, 3, 5, 5]) (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "# When we already have a tensor and want to add names (but not change existing\n",
    "# ones), we can call the method refine_names on it. Similar to indexing, the ellipsis (…)\n",
    "# allows you to leave out any number of dimensions\n",
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_names = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "print('img named: ', img_named.shape, img_named.names)\n",
    "print(\"batch named: \", batch_names.shape, batch_names.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e998615c-7a60-4825-a663-e6633b7620a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]),)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_name.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c38132-ba51-4c6b-a2ea-ceb1b6d7c2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For operations with two inputs, in addition to the usual dimension checks—whether\n",
    "# sizes are the same, or if one is 1 and can be broadcast to the other—PyTorch will now\n",
    "# check the names for us. So far, it does not automatically align dimensions, so we need\n",
    "# to do this explicitly. The method align_as returns a tensor with missing dimensions\n",
    "# added and existing ones permuted to the right order:\n",
    "weights_aligned = weighted_name.align_as(img_named)\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d41cf6d-a8ef-40c5-94de-f510296dbfcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6749, -0.7059,  0.9121, -0.1308,  1.1092],\n",
       "        [ 0.3620, -0.2653,  1.2684,  0.4156,  1.1532],\n",
       "        [ 0.5699, -0.0723,  0.4275,  0.5150, -0.4307],\n",
       "        [ 1.3816,  1.5574, -0.2110, -0.5506,  0.9862],\n",
       "        [-0.8595, -1.6642, -0.9345,  1.5479,  0.0520]],\n",
       "       names=('rows', 'columns'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functions accepting dimension arguments, like sum, also take named dimensions:\n",
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5349316e-8ba3-4fdd-bd12-27fe61dcd2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6654b59a-10b5-4e38-a2ab-6a6b28de980d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3548,  1.1896, -1.1210, -0.5517,  0.9556],\n",
       "         [ 1.0760, -1.6869,  0.2269,  0.4788,  0.4645],\n",
       "         [ 0.8012,  0.5099,  0.5212,  0.4574,  0.1634],\n",
       "         [ 0.5559,  0.1012,  0.0905,  1.7435,  2.0226],\n",
       "         [-1.0296, -0.2199, -0.4711, -0.2198,  2.6266]],\n",
       "\n",
       "        [[-0.7974, -1.3237,  1.6206,  0.1641,  0.9851],\n",
       "         [ 0.0720,  0.1864,  1.6281,  0.5755,  1.5257],\n",
       "         [ 0.3105, -0.4134,  0.5031,  0.5214, -0.6756],\n",
       "         [ 1.7749,  2.1326, -0.2099, -1.2695,  0.8215],\n",
       "         [-0.7103, -2.3042, -1.1936,  2.2267, -0.6394]],\n",
       "\n",
       "        [[-0.4040, -0.1682, -0.1198, -1.8118,  2.7900],\n",
       "         [ 1.1330, -0.5533,  0.7714, -1.3545, -0.5088],\n",
       "         [ 2.4578,  1.5919, -0.5967,  0.6212,  0.2458],\n",
       "         [-0.0823,  0.1477, -1.1103, -0.1843, -0.4339],\n",
       "         [-1.8364,  0.4226,  0.2673,  0.0290, -0.6807]]],\n",
       "       names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "212e7800-7c00-452c-b728-a84b7cca5690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed502bf3-af88-4334-8e2a-43de5752462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3548,  1.1896, -1.1210],\n",
       "         [ 1.0760, -1.6869,  0.2269],\n",
       "         [ 0.8012,  0.5099,  0.5212],\n",
       "         [ 0.5559,  0.1012,  0.0905],\n",
       "         [-1.0296, -0.2199, -0.4711]],\n",
       "\n",
       "        [[-0.7974, -1.3237,  1.6206],\n",
       "         [ 0.0720,  0.1864,  1.6281],\n",
       "         [ 0.3105, -0.4134,  0.5031],\n",
       "         [ 1.7749,  2.1326, -0.2099],\n",
       "         [-0.7103, -2.3042, -1.1936]],\n",
       "\n",
       "        [[-0.4040, -0.1682, -0.1198],\n",
       "         [ 1.1330, -0.5533,  0.7714],\n",
       "         [ 2.4578,  1.5919, -0.5967],\n",
       "         [-0.0823,  0.1477, -1.1103],\n",
       "         [-1.8364,  0.4226,  0.2673]]], names=('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_named[..., :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1abb3936-a5a4-4d08-9582-1f6845b921c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f973d249-6164-4e38-a850-60983caa164d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# If we try to combine dimensions with different names, we get an error:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m gray_named \u001b[38;5;241m=\u001b[39m (img_named \u001b[38;5;241m*\u001b[39m weighted_name)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m gray_named\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error when attempting to broadcast dims ['channels', 'rows', 'columns'] and dims ['channels']: dim 'columns' and dim 'channels' are at the same position from the right but do not match."
     ]
    }
   ],
   "source": [
    "# If we try to combine dimensions with different names, we get an error:\n",
    "gray_named = (img_named * weighted_name).sum('channels')\n",
    "gray_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc563e-6939-4f74-a4cc-0e405c42af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4db5c6a1-1664-49b0-bb96-4fa72b869630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to use tensors outside functions that operate on named tensors, we need to\n",
    "# drop the names by renaming them to None. The following gets us back into the world\n",
    "# of unnamed dimensions\n",
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7687c8a4-a6b0-4c03-9cb5-9c464a000b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing a tensor’s dtype attribute\n",
    "# In order to allocate a tensor of the right numeric type, we can specify the proper\n",
    "# dtype as an argument to the constructor. For example:\n",
    "double_points = torch.ones(10, 2, dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0acfa90f-c871-4c72-bcf8-32105ee1ca3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a82a560c-faf2-476d-9f3b-3c6f7865c4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int16"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points = torch.tensor([[1,2], [3,4]], dtype=torch.short)\n",
    "short_points.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70625cc3-ba38-47e0-a989-e1d89d64f713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also cast the output of a tensor creation function to the right type using the\n",
    "# corresponding casting method, such as\n",
    "double_points = torch.zeros(10,2).double()\n",
    "double_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb5c6084-e48f-480a-b996-3e7fe3203e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points = torch.zeros(10,5).short()\n",
    "short_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8b51e97-6824-4c1f-b685-1db3fabcf440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or the more convenient to method:\n",
    "double_points = torch.zeros(10,2).to(torch.double)\n",
    "double_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f585ca8-c3d9-4fac-be3f-813edca68905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_points = torch.zeros(10,5).to(torch.short)\n",
    "short_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e682bc0a-be07-43fc-985c-573bed80a3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9175, 0.4239, 0.6690, 0.5684, 0.1272], dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When mixing input types in operations, the inputs are converted to the larger type\n",
    "# automatically. Thus, if we want 32-bit computation, we need to make sure all our\n",
    "# inputs are (at most) 32-bit:\n",
    "points_64 = torch.rand(5).double()\n",
    "points_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1d0d268-0ee2-48db-ae3c-6896dad8b13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0], dtype=torch.int16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_short = points_64.short()\n",
    "points_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d12ee449-439b-4289-bb03-36627605f452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 * points_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bb26d58-1c26-4a89-9b7c-b2d8796ed917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4221, 0.2350],\n",
       "        [0.3681, 0.7925],\n",
       "        [0.5724, 0.1722]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tensor API\n",
    "\n",
    "# the vast majority of operations on and between tensors are available in the\n",
    "# torch module and can also be called as methods of a tensor object. For instance, the\n",
    "# transpose function we encountered earlier can be used from the torch module\n",
    "a = torch.rand(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "11e207e9-6ed7-4b45-b337-935ac51d53f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4221, 0.3681, 0.5724],\n",
       "        [0.2350, 0.7925, 0.1722]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t = torch.transpose(a, 0, 1)\n",
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09e53414-09a1-46b4-86c6-6c9397764919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2951, 0.2059],\n",
       "        [0.8454, 0.0831],\n",
       "        [0.7378, 0.4983]], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or as a method of the a tensor:\n",
    "a = torch.rand(3,2).double()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d78656b-2d4b-46de-aa69-df43fc508125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2951, 0.8454, 0.7378],\n",
       "        [0.2059, 0.0831, 0.4983]], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_t = a.transpose(0,1)\n",
    "a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf84ef29-433e-41bb-8e83-38ddceaa1cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors: Scenic views of storage\n",
    "# Indexing into storage\n",
    "# Let’s see how indexing into the storage works in practice with our 2D points. The storage for a given tensor is accessible using the \n",
    "# .storage property:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04061782-2149-45fd-b621-fbff862ba807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f74ff0f-4de1-4b94-a4ec-c87dbd2e406c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c694e510-b382-4073-b3f4-d2c359283b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even though the tensor reports itself as having three rows and two columns, the storage under the hood is a contiguous array of size 6. In this sense, the tensor just knows\n",
    "# how to translate a pair of indices into a location in the storage.\n",
    "# We can also index into a storage manually. For instance:\n",
    "points_storage = points.storage()\n",
    "points_storage[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f91c632-d229-4313-b176-1f715e0599d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcb50aad-97a7-450f-b362-9ea64dda9741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( 2.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       " tensor([[2., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can’t index a storage of a 2D tensor using two indices. The layout of a storage is\n",
    "# always one-dimensional, regardless of the dimensionality of any and all tensors that\n",
    "# might refer to it\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_storage = points.storage()\n",
    "points_storage[0] = 2\n",
    "points_storage, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cf87f3e0-14b5-4608-b862-e16751cf5a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a small\n",
    "# number of operations exist only as methods of the Tensor object. They are recognizable from a trailing underscore in their name, like zero_, \n",
    "# which indicates that the method operates in place by modifying the input instead of creating a new output tensor\n",
    "# and returning it. For instance, the zero_ method zeros out all the elements of the input.\n",
    "# Any method without the trailing underscore leaves the source tensor unchanged and\n",
    "# instead returns a new tensor:\n",
    "a = torch.ones(3,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0892bc4b-eb5c-4ec7-a37f-3534330f50dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.zero_()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7a64a5d-6ac5-4c20-9fa1-4bce9ee8091f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor metadata: Size, offset, and stride\n",
    "# The storage offset is the index in the storage corresponding to the first element in the tensor.\n",
    "# The stride is the number of elements in the storage that need to be skipped over to\n",
    "# obtain the next element along each dimension\n",
    "# Views of another tensor’s storage\n",
    "# We can get the second point in the tensor by providing the corresponding index:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d19b9111-e1c4-4162-aeb4-814d1a550e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point = points[1]\n",
    "second_point.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e0e63c7-94ee-47a0-8182-502bb87d9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting tensor has offset 2 in the storage (since we need to skip the first point,\n",
    "# which has two items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "be0fb1b3-a165-41d8-ab39-8e4e0bb707a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2]), torch.Size([2]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size is an instance of the Size class containing one element, since the tensor is one-dimensional. It’s important to note that this is the\n",
    "# same information contained in the shape property of tensor objects\n",
    "second_point.size(), second_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa171bb4-b211-4cea-8cd8-f38a98a7df7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "707f6fdc-885a-4a7f-9dcf-8db81db20b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The stride is a tuple indicating the number of elements in the storage that have to be\n",
    "# skipped when the index is increased by 1 in each dimension. For instance, our points\n",
    "# tensor has a stride of (2, 1)\n",
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd3f6295-f9c0-4506-a27b-29f8001e8ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here if we print the points:\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "28f69b51-837a-43a7-8298-2ec9426b2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see the to go from 4 to 5 -( from first row to second) we have to skip 2 numbers, that is why the first number is 2\n",
    "# as for 1, that is column wise, to go from column 1 to column 2, we dont have to skip any number. That is why it is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b9b178fd-557a-4f99-8b4f-0872588a34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing an element i, j in a 2D tensor results in accessing the storage_offset +\n",
    "# stride[0] * i + stride[1] * j element in the storage. The offset will usually be\n",
    "# zero; if this tensor is a view of a storage created to hold a larger tensor, the offset might\n",
    "# be a positive value.\n",
    "# This indirection between Tensor and Storage makes some operations inexpensive, like transposing a tensor or extracting a subtensor, because they do not lead to\n",
    "# memory reallocations. Instead, they consist of allocating a new Tensor object with a\n",
    "# different value for size, storage offset, or stride.\n",
    "# We already extracted a subtensor when we indexed a specific point and saw the\n",
    "# storage offset increasing. Let’s see what happens to the size and stride as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f5b6f9a-741d-4823-9ecc-3b1769043038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point = points[1]\n",
    "second_point.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5641dc8c-aabe-4a90-897f-12ff9b0ec2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7f80f60-fcd8-4853-b379-093c5f188abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "374df261-4f9f-48fb-80ce-01af876fbdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The bottom line is that the subtensor has one less dimension, as we would expect,\n",
    "# while still indexing the same storage as the original points tensor. This also means\n",
    "# changing the subtensor will have a side effect on the original tensor:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "second_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fb43dfd-3de2-4d0c-85dc-64bc4924df1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  3.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point[0] = 10.0\n",
    "second_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f06af33-5a02-451a-80f6-0d2ad72fb817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  1.],\n",
       "        [10.,  3.],\n",
       "        [ 2.,  1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c8a9105-8ff8-4a91-b71d-d838dd2b8d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 3.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This might not always be desirable, so we can eventually clone the subtensor into a\n",
    "# new tensor:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1].clone()\n",
    "second_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "787b849c-c445-423b-a0a6-9e24cb7ee034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.,  3.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_point[0] = 10\n",
    "second_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be2e894b-0e3f-4205-9b81-11077e1bb119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cb1dff9-5879-4303-a1b1-cfbc34378e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing without copying\n",
    "# Let’s take our points tensor, which has individual points in\n",
    "# the rows and X and Y coordinates in the columns, and turn it around so that individual points are in the columns. We take this opportunity to introduce the t function, a\n",
    "# shorthand alternative to transpose for two-dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8fc7eb-9757-428b-8d0d-f8ee96671285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e39ff89f-4fbd-42ca-b628-5ecc964a19d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking transponse and storing it in the variable\n",
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "389ea538-591a-4eed-befb-00a228949777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2558251755472, 2558251755472)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can easily verify that the two tensors share the same storage\n",
    "id(points.storage()), id(points_t.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "07620885-88b4-4ba8-9972-76c019a0b6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1), (1, 2))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# they differ only in shape and stride:\n",
    "points.stride(), points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09673ce7-6257-458c-83fa-81c5d482abca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "644f5646-485c-4737-9992-26f2896babb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transposing in higher dimensions\n",
    "# Transposing in PyTorch is not limited to matrices. We can transpose a multidimensional array \n",
    "# by specifying the two dimensions along which transposing (flipping shape\n",
    "# and stride) should occur:\n",
    "some_t = torch.ones(3,4,5)\n",
    "some_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "17ca7bf1-0a3c-4c03-bd48-f23fdcc87193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "951a9c09-5dab-4d74-a8cf-9cdbdbf3293c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t = some_t.transpose(0,1)\n",
    "transpose_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ee976f91-c049-4329-b9cc-dcae62dd16f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 5])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "31ad473a-767f-44bd-b8d3-379978edb24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 5, 1), (5, 20, 1))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_t.stride(), transpose_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8073ba20-f839-41c6-8212-912c5950ad96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contiguous tensors\n",
    "# Some tensor operations in PyTorch only work on contiguous tensors, such as view,\n",
    "# which we’ll encounter in the next chapter. In that case, PyTorch will throw an informative exception \n",
    "# and require us to call contiguous explicitly. It’s worth noting that\n",
    "# calling contiguous will do nothing (and will not hurt performance) if the tensor is\n",
    "# already contiguous.\n",
    "# In our case, points is contiguous, while its transpose is not:\n",
    "points.is_contiguous(), points_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c511ca1d-be73-41bd-bd64-9b2e143cc2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4., 1.],\n",
       "         [5., 3.],\n",
       "         [2., 1.]]),\n",
       " tensor([[4., 5., 2.],\n",
       "         [1., 3., 1.]]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can obtain a new contiguous tensor from a non-contiguous one using the contiguous method. \n",
    "# The content of the tensor will be the same, but the stride will change, as\n",
    "# will the storage:\n",
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_t = points.t()\n",
    "points, points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4efb5844-9d75-4c73-a63e-4551b6426e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( 4.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       "  4.0\n",
       "  1.0\n",
       "  5.0\n",
       "  3.0\n",
       "  2.0\n",
       "  1.0\n",
       " [torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 6],\n",
       " (2, 1),\n",
       " (1, 2))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.storage(), points.storage(), points.stride(), points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9af0c9db-2267-4fca-843f-0ab81ad58a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont = points_t.contiguous()\n",
    "points_t_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e7be8b3-10da-48f3-bb2f-33d485ce6d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9ce6959b-685b-44aa-b158-a6e7decd44be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 5.0\n",
       " 2.0\n",
       " 1.0\n",
       " 3.0\n",
       " 1.0\n",
       "[torch.storage._TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488f6f96-be7d-4e91-97a8-7dadbd4a2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to the GPU\n",
    "# PyTorch tensors also can be stored on a different kind of processor: a graphics\n",
    "# processing unit (GPU). Every PyTorch tensor can be transferred to (one of) the\n",
    "# GPU(s) in order to perform massively parallel, fast computations. All \n",
    "# operations that will be performed on the tensor will be carried out using \n",
    "# GPU-specific routines that come with PyTorch.\n",
    "# Managing a tensor’s device attribute\n",
    "# In addition to dtype, a PyTorch Tensor also has the notion of device, which is\n",
    "# where on the computer the tensor data is placed. Here is how we can create a \n",
    "# tensor on the GPU by specifying the corresponding argument to the constructor:\n",
    "# points_gpu = torch.tensor([[4, 1], [5, 3], [2, 1]], device='cuda').float()\n",
    "# points_gpu      # here the cuda is not installed, it will throw error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d1b0f45-a1a5-48a2-8fca-87794a1c894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create a copy of a tensor created on cpu on gpu\n",
    "# points_gpu = points.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65e24668-4cc6-4b26-b7ff-bdbd45f878dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If our machine has more than one GPU, we can also decide on which GPU we \n",
    "# allocate the tensor by passing a zero-based integer identifying the GPU on \n",
    "# the machine, such as\n",
    "# points_gpu = points.to(device='cuda:0') # or 1, 2, 3 -- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfe5ac6d-e934-408e-ac63-5a4c051bb9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use the shorthand methods cpu and cuda instead of the to method to\n",
    "# achieve the same goal:\n",
    "# points_gpu = points.cuda() # default GPU == 0\n",
    "# points_gpu = points.cuda(0) # default GPU == 1\n",
    "# points_cpu = points_gpu.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6daa79a-c42d-46f8-8800-c3b87f0d0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s also worth mentioning that by using the to method, we can change the \n",
    "# placement and the data type simultaneously by providing both device and \n",
    "# dtype as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "416cf006-d254-48ea-b811-aa719adf40e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy interoperability\n",
    "# PyTorch tensors can be converted to NumPy arrays and vice versa very efficientl\n",
    "# To get a NumPy array out of our points tensor, we just call\n",
    "points = torch.ones(3,4)\n",
    "points_np = points.numpy()\n",
    "points_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22d3883e-7f3c-479f-ad6d-7f56349ab524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the returned array shares the same underlying buffer with the tensor storage. \n",
    "# This means the numpy method can be effectively executed at\n",
    "# basically no cost, as long as the data sits in CPU RAM.\n",
    "# It also means modifying the NumPy array will lead to a change in the \n",
    "# originating tensor\n",
    "# Conversely, we can obtain a PyTorch tensor from a NumPy array this way\n",
    "points = torch.from_numpy(points_np)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33c5962c-0793-4b70-98b6-7b67f628aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While the default numeric type in PyTorch is 32-bit floating-point, for\n",
    "# NumPy it is 64-bit. As discussed in section 3.5.2, we usually want to use 32-bit\n",
    "# floating-points, so we need to make sure we have tensors of dtype torch\n",
    "# .float after converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00af1dbf-d411-4823-bd8f-6c181f347e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1,), 0, torch.Size([9]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execises\n",
    "a = torch.tensor(list(range(9)))\n",
    "a.stride(), a.storage_offset(), a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3399a87b-9e23-4a0e-a2d2-32762aed96e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2417431613488"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2b136aa-7220-468e-8f8f-b61545e3cec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(3,3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daab797d-e13b-4893-aeb9-ff32a17d424e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), 0, torch.Size([3, 3]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), b.storage_offset(), b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64ce83e8-bc52-4a6a-9cd9-1e2a57b86984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2417313429232"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "081293cc-a502-4597-9014-88740cea61d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5],\n",
       "        [7, 8]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b[1:, 1:]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4432801a-f262-4d04-aa34-2415e80deac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), 4, torch.Size([2, 2]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.stride(), c.storage_offset(), c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ba713f6-5973-4595-8885-ed40d51b571a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       "[torch.storage._TypedStorage(dtype=torch.int64, device=cpu) of size 9]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "556f8dbf-ce37-442b-95bd-f9a0ac9ab70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 1.0000, 1.4142, 1.7321, 2.0000, 2.2361, 2.4495, 2.6458, 2.8284])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.sqrt(a)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c435b-c5ae-46c6-8223-505beb8eda9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
