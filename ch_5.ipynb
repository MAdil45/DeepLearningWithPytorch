{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a1273b-9214-4897-98ad-ba0b8d8dc04b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The mechanics of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2b67e-3ac8-4631-a38c-4f1fb5461f6c",
   "metadata": {},
   "source": [
    "This notebook covers\n",
    "- Understanding how algorithms can learn from data\n",
    "- Reframing learning as parameter estimation, using\n",
    "differentiation and gradient descent\n",
    "- Walking through a simple learning algorithm\n",
    "- How PyTorch supports learning with autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceef872-2224-432b-a743-d773b3209def",
   "metadata": {},
   "source": [
    "A learning algorithm is presented with\n",
    "input data that is paired with desired outputs. Once learning has occurred, that\n",
    "algorithm will be capable of producing correct outputs when it is fed new data that\n",
    "is similar enough to the input data it was trained on. With deep learning, this process\n",
    "works even when the input data and the desired output are far from each other:\n",
    "when they come from different domains, like an image and a sentence describing\n",
    "it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde0e5d-84e9-4c78-97ce-6d7bc2ce4d39",
   "metadata": {},
   "source": [
    "## A timeless lesson in modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40caf7af-429b-4574-9730-e83c3648afae",
   "metadata": {},
   "source": [
    "Building models that allow us to explain input/output relationships dates back centuries at least. When Johannes Kepler, a German mathematical astronomer (1571–1630),\n",
    "figured out his three laws of planetary motion in the early 1600s, he based them on\n",
    "data collected by his mentor Tycho Brahe during naked-eye observations (yep, seen\n",
    "with the naked eye and written on a piece of paper). Not having Newton’s law of gravitation at his disposal (actually, Newton used Kepler’s work to figure things out),\n",
    "Kepler extrapolated the simplest possible geometric model that could fit the data.\n",
    "And, by the way, it took him six years of staring at data that didn’t make sense to him,\n",
    "together with incremental realizations, to finally formulate these laws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da5f2c8-7169-49c0-98f2-7f8c37191f82",
   "metadata": {},
   "source": [
    "Over six years, Kepler\n",
    "- Got lots of good data from his friend Brahe (not without some struggle)\n",
    "- Tried to visualize the heck out of it, because he felt there was something fishy\n",
    "going on\n",
    "- Chose the simplest possible model that had a chance to fit the data (an ellipse)\n",
    "- Split the data so that he could work on part of it and keep an independent set\n",
    "for validation\n",
    "- Started with a tentative eccentricity and size for the ellipse and iterated until the\n",
    "model fit the observations\n",
    "- Validated his model on the independent observations\n",
    "- Looked back in disbelief"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fe897-99e6-4133-a9ee-f2e8a9d88739",
   "metadata": {},
   "source": [
    "There’s a data science handbook for you, all the way from 1609. The history of science\n",
    "is literally constructed on these seven steps. And we have learned over the centuries\n",
    "that deviating from them is a recipe for disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51932d6-344c-45a2-84bd-53d7800b9b86",
   "metadata": {},
   "source": [
    "This is exactly what we will set out to do in order to learn something from data. In\n",
    "fact, in this book there is virtually no difference between saying that we’ll fit the data\n",
    "or that we’ll make an algorithm learn from data. The process always involves a function with a number of unknown parameters whose values are estimated from data: in\n",
    "short, a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056bb169-bb88-4f93-a8c4-c4c5eeba97be",
   "metadata": {},
   "source": [
    "We can argue that learning from data presumes the underlying model is not engineered to solve a specific problem (as was the ellipse in Kepler’s work) and is instead\n",
    "capable of approximating a much wider family of functions. A neural network would\n",
    "have predicted Tycho Brahe’s trajectories really well without requiring Kepler’s flash\n",
    "of insight to try fitting the data to an ellipse. However, Sir Isaac Newton would have\n",
    "had a much harder time deriving his laws of gravitation from a generic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2b811-d840-4333-98ee-e0ab5f90c87d",
   "metadata": {},
   "source": [
    "## Learning is just parameter estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2935043d-a45a-4b95-9927-ce60aca69f7a",
   "metadata": {},
   "source": [
    "Given input data and the corresponding desired outputs (ground truth), as\n",
    "well as initial values for the weights, the model is fed input data (forward pass), and a\n",
    "measure of the error is evaluated by comparing the resulting outputs to the ground\n",
    "truth. In order to optimize the parameter of the model—its weights—the change in\n",
    "the error following a unit change in weights (that is, the gradient of the error with\n",
    "respect to the parameters) is computed using the chain rule for the derivative of a\n",
    "composite function (backward pass). The value of the weights is then updated in the\n",
    "direction that leads to a decrease in the error. The procedure is repeated until the\n",
    "error, evaluated on unseen data, falls below an acceptable level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50f908-b895-4d95-99cb-343879d7d5e2",
   "metadata": {},
   "source": [
    "### A hot Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6263ca-ec66-4861-9360-48c1d0909799",
   "metadata": {},
   "source": [
    "We just got back from a trip to some obscure location, and we brought back a fancy,\n",
    "wall-mounted analog thermometer. It looks great, and it’s a perfect fit for our living\n",
    "room. Its only flaw is that it doesn’t show units. Not to worry, we’ve got a plan: we’ll\n",
    "build a dataset of readings and corresponding temperature values in our favorite\n",
    "units, choose a model, adjust its weights iteratively until a measure of the error is low\n",
    "enough, and finally be able to interpret the new readings in units we understand.4\n",
    "Let’s try following the same process Kepler used. Along the way, we’ll use a tool he\n",
    "never had available: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4ee8e-6df2-4bd0-9768-4c10bcd97f69",
   "metadata": {},
   "source": [
    "### Gathering some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2762a84-8770-4a4a-b504-3f754b0fc200",
   "metadata": {},
   "source": [
    "We’ll start by making a note of temperature data in good old Celsius5 and measurements from our new thermometer, and figure things out. After a couple of weeks,\n",
    "here’s the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c31561-3843-440f-9088-1d0aa88ca884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.adil\\Miniconda3\\envs\\sahi\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24861d91-2610-434f-a03a-6a24f2e50726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)\n",
    "t_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee45ac-ed57-4b6c-a5cf-85943ca27e59",
   "metadata": {},
   "source": [
    "Here, the t_c values are temperatures in Celsius, and the t_u values are our unknown\n",
    "units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. For convenience, we’ve already put the\n",
    "data into tensors; we’ll use it in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878cdd7-063d-4c6e-af67-33095982558c",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d384bef-78a8-4671-89db-cb130a3ca876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x260ae2e11b0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 704x528 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(dpi=110)\n",
    "plt.xlabel('Measurements')\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u, t_c, 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cd8c8-f215-4f39-a733-3f730fb39974",
   "metadata": {},
   "source": [
    "A quick plot of our data tells us that it’s noisy, but we think there’s a pattern here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e9764-43cc-4253-b652-3cd6a8ea5542",
   "metadata": {},
   "source": [
    "### Choosing a linear model as a first try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783bcf3-3717-4227-a952-34289ec765d9",
   "metadata": {},
   "source": [
    "In the absence of further knowledge, we assume the simplest possible model for converting between the two sets of measurements, just like Kepler might have done. The\n",
    "two may be linearly related—that is, multiplying t_u by a factor and adding a constant,\n",
    "we may get the temperature in Celsius "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387b2d1-fe0c-473b-9186-44ea62eb7161",
   "metadata": {},
   "source": [
    "- ##### t_c = w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049c333-10b6-4ec3-bda4-befeb0210404",
   "metadata": {},
   "source": [
    "We chose to name w and b after weight and bias, two very common terms for linear scaling and the additive constant—we’ll bump into those all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492c12b-e2c5-48eb-8691-8b3c5c28b6ca",
   "metadata": {},
   "source": [
    "OK, now we need to estimate w and b, the parameters in our model, based on the data\n",
    "we have. We must do it so that temperatures we obtain from running the unknown temperatures t_u through the model are close to temperatures we actually measured in Celsius. If that sounds like fitting a line through a set of measurements, well, yes, because\n",
    "that’s exactly what we’re doing. We’ll go through this simple example using PyTorch and\n",
    "realize that training a neural network will essentially involve changing the model for a\n",
    "slightly more elaborate one, with a few (or a metric ton) more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23638f98-62f2-48fc-a7e9-f82b32db3080",
   "metadata": {},
   "source": [
    "Let’s flesh it out again: we have a model with some unknown parameters, and we\n",
    "need to estimate those parameters so that the error between predicted outputs and\n",
    "measured values is as low as possible. We notice that we still need to exactly define a\n",
    "measure of the error. Such a measure, which we refer to as the loss function, should be\n",
    "high if the error is high and should ideally be as low as possible for a perfect match.\n",
    "Our optimization process should therefore aim at finding w and b so that the loss\n",
    "function is at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602f296-d519-42b4-95ea-8ea6b0b4b051",
   "metadata": {},
   "source": [
    "## Less loss is what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f710c21-ee0f-4d13-a109-91da6ea82a0f",
   "metadata": {},
   "source": [
    "A loss function (or cost function) is a function that computes a single numerical value\n",
    "that the learning process will attempt to minimize. The calculation of loss typically\n",
    "involves taking the difference between the desired outputs for some training samples\n",
    "and the outputs actually produced by the model when fed those samples. In our case,\n",
    "that would be the difference between the predicted temperatures t_p output by our\n",
    "model and the actual measurements: t_p – t_c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b7484-7c33-4e7a-8d22-187a945d9afd",
   "metadata": {},
   "source": [
    "We need to make sure the loss function makes the loss positive both when t_p is\n",
    "greater than and when it is less than the true t_c, since the goal is for t_p to match t_c.\n",
    "We have a few choices, the most straightforward being |t_p – t_c| and (t_p – t_c)^2.\n",
    "Based on the mathematical expression we choose, we can emphasize or discount certain\n",
    "errors. Conceptually, a loss function is a way of prioritizing which errors to fix from our\n",
    "training samples, so that our parameter updates result in adjustments to the outputs for\n",
    "the highly weighted samples instead of changes to some other samples’ output that had\n",
    "a smaller loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a6fe6-7fa3-41c9-8f0a-1a5eb2cad2bb",
   "metadata": {},
   "source": [
    "Both of the example loss functions have a clear minimum at zero and grow monotonically as the predicted value moves further from the true value in either direction.\n",
    "Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be convex. Since our model is linear, the loss as a function\n",
    "of w and b is also convex. Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficiently of w and b is also convex. Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficiently "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c707b9-52db-4b4f-89cc-3614d67a578d",
   "metadata": {},
   "source": [
    "For our two loss functions |t_p – t_c| and (t_p – t_c)^2, the square of the differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p\n",
    "equals t_c. The absolute value, on the other hand, has an undefined derivative right\n",
    "where we’d like to converge. This is less of an issue in practice than it looks like, but\n",
    "we’ll stick to the square of differences for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3f2cd-8443-498a-bc2b-9b144bff5a14",
   "metadata": {},
   "source": [
    "It’s worth noting that the square difference also penalizes wildly wrong results more than\n",
    "the absolute difference does. Often, having more slightly wrong results is better than having a few wildly wrong ones, and the squared difference helps prioritize those as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3247ec8-ba8e-4c47-bc32-09ce03aead06",
   "metadata": {},
   "source": [
    "### From problem back to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1ae4f-3447-41a6-8bfc-0470f0b90074",
   "metadata": {},
   "source": [
    "we’ve already got a good part of\n",
    "the high-level and now we need to set the learning process in motion and feed it actual data. Also, enough with math notation; let’s switch to\n",
    "PyTorch—after all, we came here for the fun.\n",
    "We’ve already created our data tensors, so now let’s write out the model as a\n",
    "Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80d767d-61be-4793-9a04-13b4318f6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return (w * t_u + b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc590ac-bc7b-4ea5-aef6-2d79a251f8cd",
   "metadata": {},
   "source": [
    "We’re expecting t_u, w, and b to be the input tensor, weight parameter, and bias\n",
    "parameter, respectively. In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield\n",
    "the returned tensors. Anyway, time to define our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32fc2b96-3ff0-41cc-9843-a877431cfd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_c, t_p):\n",
    "    squarred_diff = (t_c - t_p)**2\n",
    "    return squarred_diff.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aadffd-0442-441c-a2e0-33ac26052e19",
   "metadata": {},
   "source": [
    "Note that we are building a tensor of differences, taking their square element-wise,\n",
    "and finally producing a scalar loss function by averaging all of the elements in the\n",
    "resulting tensor. It is a mean square loss.\n",
    "We can now initialize the parameters, invoke the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2823a58-6f06-4858-bc1f-0fb01ccddf69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c666cc2-8ea5-4733-a1e8-f38e45270181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros(())\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e458a68-d36e-4938-ac67-bd308642a31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f68321-8a79-4488-bf35-fdcf6afcf7a1",
   "metadata": {},
   "source": [
    "and check the value of the loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb5c995-ba3d-4981-8fd8-b13ff5da7f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_c, t_p)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642b9b89-b89c-478c-8af5-2209e019b451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acc4be51-4ada-4d10-a8c6-e1e1f097dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13d8450-f02e-4297-bd4e-8efb6498bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((t_c - t_p)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668ddf3-7367-4315-ae2d-c985dba83933",
   "metadata": {},
   "source": [
    "We implemented the model and the loss in this section. We’ve finally reached the\n",
    "meat of the example: how do we estimate w and b such that the loss reaches a minimum? We’ll first work things out by hand and then learn how to use PyTorch’s superpowers to solve the same problem in a more general, off-the-shelf way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b3427c-adf5-45de-b1db-eeee249b4881",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eae65b-9c85-4ff1-9478-2282cef5cf97",
   "metadata": {},
   "source": [
    "Usually—and in early versions of PyTorch, too—we can only use element-wise binary\n",
    "operations such as addition, subtraction, multiplication, and division for arguments\n",
    "of the same shape. The entries in matching positions in each of the tensors will be\n",
    "used to calculate the corresponding entry in the result tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e93c5-17b7-4435-847d-54ab423a9a49",
   "metadata": {},
   "source": [
    "Broadcasting, which is popular in NumPy and adapted by PyTorch, relaxes this assumption for most binary operations. It uses the following rules to match tensor elements:\n",
    "- For each index dimension, counted from the back, if one of the operands is\n",
    "size 1 in that dimension, PyTorch will use the single entry along this dimension with each of the entries in the other tensor along this dimension.\n",
    "- If both sizes are greater than 1, they must be the same, and natural matching\n",
    "is used.\n",
    "- If one of the tensors has more index dimensions than the other, the entirety\n",
    "of the other tensor will be used for each entry along these dimensions.\n",
    "\n",
    "Of course, this would all be theory if we didn’t have some code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade0afb3-bf5a-4f1a-b069-21655bd18c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.), torch.Size([]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(())\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d191cfeb-4f17-4404-9c4c-229d0b5df786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones((3,1))\n",
    "y, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f7dee0-7ce9-45fb-a16c-2872ed453ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.]]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones((1,3))\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a51bd44-d92f-41d8-ba2f-237f32669ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.]],\n",
       " \n",
       "         [[1.]]]),\n",
       " torch.Size([2, 1, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((2,1,1))\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7639f75-269b-474a-a699-38c8add5eca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y, (x * y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c436a96-230b-42a3-8fbf-51bf87e39338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y*z, (y*z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88495064-b6ba-42e8-86b5-222dc8098241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]),\n",
       " torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y * z * a, (y * z * a).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4059b5f-79ac-46f0-a5aa-299eb46b014b",
   "metadata": {},
   "source": [
    "## Down along the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1085c43-0812-4b82-9b0c-d9ff6371adcc",
   "metadata": {},
   "source": [
    "We’ll optimize the loss function with respect to the parameters using the gradient\n",
    "descent algorithm. In this section, we’ll build our intuition for how gradient descent\n",
    "works from first principles, which will help us a lot in the future. As we mentioned,\n",
    "there are ways to solve our example problem more efficiently, but those approaches\n",
    "aren’t applicable to most deep learning tasks. Gradient descent is actually a very simple idea, and it scales up surprisingly well to large neural network models with millions of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780265ac-04b8-4b2d-b4fb-4bb594b48de9",
   "metadata": {},
   "source": [
    "### Decreasing loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63635dd5-9145-4e8d-b654-133a4ee3bd57",
   "metadata": {},
   "source": [
    "The idea is\n",
    "to compute the rate of change of the loss with respect to each parameter, and modify\n",
    "each parameter in the direction of decreasing loss. We can estimate the rate of change by adding a small number to w and\n",
    "b and seeing how much the loss changes in that neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265cf2c5-fb3c-4d61-9e4f-6c32c7d7d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e85cd17d-cfd5-426b-b175-698c37d3bd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_w = (loss_fn(model(t_u, w+delta, b), t_c) - loss_fn(model(t_u, w-delta, b), t_c))/(2 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03e3635a-33d9-45c4-aa47-2768fc9988fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2974)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ea8c8-52d4-42c8-981f-ace98e5c8d83",
   "metadata": {},
   "source": [
    "This is saying that in the neighborhood of the current values of w and b, a unit\n",
    "increase in w leads to some change in the loss. If the change is negative, then we need\n",
    "to increase w to minimize the loss, whereas if the change is positive, we need to\n",
    "decrease w. By how much? Applying a change to w that is proportional to the rate of\n",
    "change of the loss is a good idea, especially when the loss has several parameters: we\n",
    "apply a change to those that exert a significant change on the loss. It is also wise to\n",
    "change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current w value. Therefore,\n",
    "we typically should scale the rate of change by a small factor. This scaling factor has\n",
    "many names; the one we use in machine learning is learning_rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21a6cc2f-7c0e-463a-b83b-950ef5a37225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a246c95-668d-4b0d-96ba-5f8d180f24e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.1730)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43112e6e-301c-4bd5-be0b-e68b8bb02467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4600.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c))/( 2 * delta)\n",
    "loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86345da4-0bd2-4bfb-9803-f5965c70a256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46.)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b - (learning_rate * loss_rate_of_change_b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2a77dfc-dd97-445f-9939-84851473570f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5589329.5000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(model(t_u, w, b), t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629684f-6a9f-4d96-8581-55c75482dd9e",
   "metadata": {},
   "source": [
    "This represents the basic parameter-update step for gradient descent. By reiterating\n",
    "these evaluations (and provided we choose a small enough learning rate), we will\n",
    "converge to an optimal value of the parameters for which the loss computed on the\n",
    "given data is minimal. We’ll show the complete iterative process soon, but the way we\n",
    "just computed our rates of change is rather crude and needs an upgrade before we\n",
    "move on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3352d-e4d1-4fb6-be56-2aa264472a5b",
   "metadata": {},
   "source": [
    "### Getting Analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee468c2-e9fc-4dc0-a9cf-c358c5f42415",
   "metadata": {},
   "source": [
    "Computing the rate of change by using repeated evaluations of the model and loss in\n",
    "order to probe the behavior of the loss function in the neighborhood of w and b\n",
    "doesn’t scale well to models with many parameters. Also, it is not always clear how\n",
    "large the neighborhood should be. We chose delta equal to 0.1 in the previous section, but it all depends on the shape of the loss as a function of w and b. If the loss\n",
    "changes too quickly compared to delta, we won’t have a very good idea of in which\n",
    "direction the loss is decreasing the most.\n",
    "What if we could make the neighborhood infinitesimally small?\n",
    "That’s exactly what happens when we analytically take the derivative of the loss with\n",
    "respect to a parameter. In a model with two or more parameters like the one we’re\n",
    "dealing with, we compute the individual derivatives of the loss with respect to each\n",
    "parameter and put them in a vector of derivatives: the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bd0022-29a2-47cd-b8d2-7b0bb4afecd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**(Computing the Derivative)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e08c1a-d0c6-41f7-ac92-07123de61378",
   "metadata": {},
   "source": [
    "In order to compute the derivative of the loss with respect to a parameter, we can\n",
    "apply the chain rule and compute the derivative of the loss with respect to its input\n",
    "(which is the output of the model), times the derivative of the model with respect to\n",
    "the parameter:\n",
    "\n",
    "- **(d loss_fn/d w = (d loss_fn / d t_p) * (d t_p / d w))**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33eaf69-8453-477a-a441-99bab331f9ec",
   "metadata": {},
   "source": [
    "Recall that our model is a linear function, and our loss is a sum of squares. Let’s figure\n",
    "out the expressions for the derivatives. Recalling the expression for the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d94c0cef-5b44-45e4-931e-6c3943d2f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squarred_diffs = (t_p - t_c) ** 2\n",
    "    return squarred_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bebde-6452-44c1-9de5-0fb0400d6fc5",
   "metadata": {},
   "source": [
    "Remembering that __(d x^2 / d x = 2 x),__ we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0df7886-f0e3-4382-9e4a-90ed076473bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "561920ce-4b48-4b64-91b1-31f09da72980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(tp, tc):\n",
    "    dsq_diffs = 2 * (tp - tc)/tp.size(0)\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20a7b2-b3f7-4306-9079-b9ff9966beef",
   "metadata": {},
   "source": [
    "**(APPLYING THE DERIVATIVES TO THE MODEL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f699514-2e5b-44e3-ab6a-bd7d1f92eb8b",
   "metadata": {},
   "source": [
    "For the model, recalling that our model is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70ba48f4-03f9-4705-bc6c-ef3f298f8dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return (w * t_u + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abec2b-b7e8-44be-ade9-31dee2ff3c78",
   "metadata": {},
   "source": [
    "we get these derivatives:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5bc69d3-808e-41a4-b924-e80d6b4085ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3afe87f-43be-4e61-87f9-02981884023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88fdd0-bca6-4075-a4d3-e85c2d3f0216",
   "metadata": {},
   "source": [
    "**(DEFINING THE GRADIENT FUNCTION)**\n",
    "\n",
    "Putting all of this together, the function returning the gradient of the loss with respect\n",
    "to w and b is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50af5f63-e94d-43dd-ab7a-9e8af6193420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1871c0-ca41-4cc6-984a-94d3b5556dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Iterating to fit the model**\n",
    "\n",
    "We now have everything in place to optimize our parameters. Starting from a tentative\n",
    "value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing. There are several stopping criteria; for now,\n",
    "we’ll stick to a fixed number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f750de77-cec3-449e-8c25-8fef7f11f6d1",
   "metadata": {},
   "source": [
    "**THE TRAINING LOOP**\n",
    "\n",
    "Since we’re at it, let’s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an epoch.\n",
    "\n",
    "The complete training loop looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a423d1a-eda7-49a6-854a-105e51ee57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w,b = params\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss =loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        params = params - learning_rate * grad\n",
    "        print(\" Epoch %d, Loss %f\" % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a189b0-e4f3-4f9c-bc6f-8088dbdabfb8",
   "metadata": {},
   "source": [
    "Now, let’s invoke our training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff40e6f0-16de-43f5-a11c-da3a5f0be69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n",
    "                  print_params=True):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "\n",
    "        t_p = model(t_u, w, b)  # <1>\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "\n",
    "        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            if print_params:\n",
    "                print('    Params:', params)\n",
    "                print('    Grad:  ', grad)\n",
    "        if epoch in {4, 12, 101}:\n",
    "            print('...')\n",
    "\n",
    "        if not torch.isfinite(loss).all():\n",
    "            break  # <3>\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f172ba4-630a-4631-b9b6-c779e6f0e2f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "    Params: tensor([-44.1730,  -0.8260])\n",
      "    Grad:   tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 5802484.500000\n",
      "    Params: tensor([2568.4011,   45.1637])\n",
      "    Grad:   tensor([-261257.4062,   -4598.9702])\n",
      "Epoch 3, Loss 19408029696.000000\n",
      "    Params: tensor([-148527.7344,   -2616.3931])\n",
      "    Grad:   tensor([15109614.0000,   266155.6875])\n",
      "...\n",
      "Epoch 10, Loss 90901105189019073810297959556841472.000000\n",
      "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 11, Loss inf\n",
      "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.8590e+19, -3.2746e+17])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=100, learning_rate=1e-2, params=torch.tensor([1.0, 0.0]), t_u=t_u, t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34847d-b9a1-436d-a7a8-f71af45e814f",
   "metadata": {},
   "source": [
    "***OVERTRAINING***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe99d8-bf50-4c07-9c36-bb0af68e571c",
   "metadata": {},
   "source": [
    "Wait, what happened? Our training process literally blew up, leading to losses becoming inf. This is a clear sign that params is receiving updates that are too large, and\n",
    "their values start oscillating back and forth as each update overshoots and the next\n",
    "overcorrects even more. The optimization process is unstable: it diverges instead of\n",
    "converging to a minimum. We want to see smaller and smaller updates to params, not\n",
    "larger, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433c852-ec3b-40b3-8038-3a2468032da3",
   "metadata": {},
   "source": [
    "How can we limit the magnitude of learning_rate * grad? Well, that looks easy. We\n",
    "could simply choose a smaller learning_rate, and indeed, the learning rate is one of\n",
    "the things we typically change when training does not go as well as we would like.8 We\n",
    "usually change learning rates by orders of magnitude, so we might try with 1e-3 or\n",
    "1e-4, which would decrease the magnitude of the updates by orders of magnitude.\n",
    "Let’s go with 1e-4 and see how it works out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d4fcef3-ae5d-4663-bbb5-694460fcf3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "    Params: tensor([ 0.5483, -0.0083])\n",
      "    Grad:   tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 323.090515\n",
      "    Params: tensor([ 0.3623, -0.0118])\n",
      "    Grad:   tensor([1859.5493,   35.7843])\n",
      "Epoch 3, Loss 78.929634\n",
      "    Params: tensor([ 0.2858, -0.0135])\n",
      "    Grad:   tensor([765.4666,  16.5122])\n",
      "...\n",
      "Epoch 10, Loss 29.105247\n",
      "    Params: tensor([ 0.2324, -0.0166])\n",
      "    Grad:   tensor([1.4803, 3.0544])\n",
      "Epoch 11, Loss 29.104168\n",
      "    Params: tensor([ 0.2323, -0.0169])\n",
      "    Grad:   tensor([0.5781, 3.0384])\n",
      "...\n",
      "Epoch 99, Loss 29.023582\n",
      "    Params: tensor([ 0.2327, -0.0435])\n",
      "    Grad:   tensor([-0.0533,  3.0226])\n",
      "Epoch 100, Loss 29.022667\n",
      "    Params: tensor([ 0.2327, -0.0438])\n",
      "    Grad:   tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=100, learning_rate=1e-4, params=torch.tensor([1.0, 0.0]), t_u=t_u, t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb4cfe-649e-4880-84bd-4a3694946b7c",
   "metadata": {},
   "source": [
    "Nice—the behavior is now stable. But there’s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could\n",
    "obviate this issue by making learning_rate adaptive: that is, change according to the\n",
    "magnitude of updates. There are optimization schemes that do that, and we’ll see one\n",
    "toward the end of this chapter, in section 5.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f381028-0e9d-4227-90c2-9649f63a948c",
   "metadata": {},
   "source": [
    "### Normalizing the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1f298-0152-46d9-bcac-039f2d3bc334",
   "metadata": {},
   "source": [
    "We can see that the first-epoch gradient for the weight is about 50 times larger than\n",
    "the gradient for the bias. This means the weight and bias live in differently scaled\n",
    "spaces. If this is the case, a learning rate that’s large enough to meaningfully update\n",
    "one will be so large as to be unstable for the other; and a rate that’s appropriate for\n",
    "the other won’t be large enough to meaningfully change the first. That means we’re\n",
    "not going to be able to update our parameters unless we change something about our\n",
    "formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with; it’s\n",
    "babysitting of the kind we don’t like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc1edad-7815-4b2f-83d9-159033f75769",
   "metadata": {},
   "source": [
    "There’s a simpler way to keep things in check: changing the inputs so that the gradients aren’t quite so different. We can make sure the range of the input doesn’t get\n",
    "too far from the range of –1.0 to 1.0, roughly speaking. In our case, we can achieve\n",
    "something close enough to that by simply multiplying t_u by 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab9e0b91-e5ee-4b08-8dee-7766acae2375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5700, 5.5900, 5.8200, 8.1900, 5.6300, 4.8900, 3.3900, 2.1800, 4.8400,\n",
       "        6.0400, 6.8400])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_un = t_u * 0.1\n",
    "t_un"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4343da-c20a-4257-b5eb-9302442a219c",
   "metadata": {},
   "source": [
    "Here, we denote the normalized version of t_u by appending an n to the variable\n",
    "name. At this point, we can run the training loop on our normalized input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11293235-0a13-430a-8a06-e29a6ab66ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574913\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587])\n",
      "...\n",
      "Epoch 10, Loss 29.030489\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941877\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264])\n",
      "...\n",
      "Epoch 99, Loss 22.214186\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=100, learning_rate=1e-2, params=torch.tensor([1.0, 0.0]), t_u = t_un, t_c=t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5ffec-6c64-4f61-8c90-7b6cf8acadf8",
   "metadata": {},
   "source": [
    "Even though we set our learning rate back to 1e-2, parameters don’t blow up during\n",
    "iterative updates. Let’s take a look at the gradients: they’re of similar magnitude, so\n",
    "using a single learning_rate for both parameters works just fine. We could probably\n",
    "do a better job of normalization than a simple rescaling by a factor of 10, but since\n",
    "doing so is good enough for our needs, we’re going to stick with that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9690f61-e2ff-4315-bb45-8a10f258352d",
   "metadata": {},
   "source": [
    "**NOTE** \n",
    "\n",
    "The normalization here absolutely helps get the network trained, but\n",
    "you could make an argument that it’s not strictly needed to optimize the\n",
    "parameters for this particular problem. That’s absolutely true! This problem is\n",
    "small enough that there are numerous ways to beat the parameters into submission. However, for larger, more sophisticated problems, normalization is an\n",
    "easy and effective (if not crucial!) tool to use to improve model convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a87d3f-b1d5-44f5-bd2b-eb267df2f993",
   "metadata": {},
   "source": [
    "Let’s run the loop for enough iterations to see the changes in params get small. We’ll\n",
    "change n_epochs to 5,000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a9de453-744e-4407-a6f8-789770d05522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 187.386368\n",
      "Epoch 2, Loss 54.594688\n",
      "Epoch 3, Loss 33.950424\n",
      "...\n",
      "Epoch 10, Loss 29.543169\n",
      "Epoch 11, Loss 29.452814\n",
      "...\n",
      "Epoch 99, Loss 22.592989\n",
      "Epoch 100, Loss 22.526230\n",
      "...\n",
      "Epoch 4000, Loss 2.927681\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(n_epochs=5000, learning_rate=1e-2, params=torch.tensor([0, 0]), t_u=t_un, t_c=t_c, print_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69a96b2-bc00-4c41-86ed-06f47910d4fd",
   "metadata": {},
   "source": [
    "Good: our loss decreases while we change parameters along the direction of gradient\n",
    "descent. It doesn’t go exactly to zero; this could mean there aren’t enough iterations to\n",
    "converge to zero, or that the data points don’t sit exactly on a line. As we anticipated, our\n",
    "measurements were not perfectly accurate, or there was noise involved in the reading.\n",
    "But look: the values for w and b look an awful lot like the numbers we need to use\n",
    "to convert Celsius to Fahrenheit (after accounting for our earlier normalization when\n",
    "we multiplied our inputs by 0.1). The exact values would be w=5.5556 and b=-\n",
    "17.7778. Our fancy thermometer was showing temperatures in Fahrenheit the whole\n",
    "time. No big discovery, except that our gradient descent optimization process works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7a636-9699-4992-8d08-a8927bd12e0f",
   "metadata": {},
   "source": [
    "### Visualizing (again)\n",
    "\n",
    "Let’s revisit something we did right at the start: plotting our data. Seriously, this is the\n",
    "first thing anyone doing data science should do. Always plot the heck out of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b0ae1d3-d097-4c5f-9444-d16a1e82dc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8593, 12.7008, 13.9352, 26.6552, 12.9155,  8.9439,  0.8932, -5.6009,\n",
       "         8.6755, 15.1160, 19.4097])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_un, *params)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "595c0ef6-9904-4304-922a-fac0572d885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.859312  , 12.7008095 , 13.935238  , 26.655216  , 12.915493  ,\n",
       "        8.943855  ,  0.89323616, -5.6009283 ,  8.675501  , 15.115997  ,\n",
       "       19.409657  ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38d770d4-2807-476c-97a9-9a8bc402b16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x260ae824910>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 768x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(dpi=120)\n",
    "plt.xlabel('Temperature (°Fahrenheit)')\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fc6de-1556-4c62-9282-d5a1033399ad",
   "metadata": {},
   "source": [
    "## PyTorch’s autograd: Backpropagating all things"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911a215-868e-419b-a071-f3ca0694e7fe",
   "metadata": {},
   "source": [
    "In our little adventure, we just saw a simple example of backpropagation: we computed the gradient of a composition of functions—the model and the loss—with\n",
    "respect to their innermost parameters (w and b) by propagating derivatives backward\n",
    "using the chain rule. The basic requirement here is that all functions we’re dealing\n",
    "with can be differentiated analytically. If this is the case, we can compute the gradient—what we earlier called “the rate of change of the loss”—with respect to the\n",
    "parameters in one sweep.\n",
    "\n",
    "Even if we have a complicated model with millions of parameters, as long as our\n",
    "model is differentiable, computing the gradient of the loss with respect to the parameters amounts to writing the analytical expression for the derivatives and evaluating\n",
    "them once. Granted, writing the analytical expression for the derivatives of a very deep\n",
    "composition of linear and nonlinear functions is not a lot of fun. It isn’t particularly\n",
    "quick, either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de218c18-92ea-49c7-b77e-666568accf42",
   "metadata": {},
   "source": [
    "### Computing the gradient automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d50d1b-1691-471e-bb46-038fb4aa8ca1",
   "metadata": {},
   "source": [
    "This is when PyTorch tensors come to the rescue, with a PyTorch component called\n",
    "autograd. Chapter 3 presented a comprehensive overview of what tensors are and what\n",
    "functions we can call on them. We left out one very interesting aspect, however:\n",
    "PyTorch tensors can remember where they come from, in terms of the operations and\n",
    "parent tensors that originated them, and they can automatically provide the chain of\n",
    "derivatives of such operations with respect to their inputs. This means we won’t need\n",
    "to derive our model by hand; given a forward expression, no matter how nested,\n",
    "PyTorch will automatically provide the gradient of that expression with respect to its\n",
    "input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27473490-4143-4a58-a94d-7f76b80a2df6",
   "metadata": {},
   "source": [
    "**APPLYING AUTOGRAD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d9662-78dc-44eb-b720-c4f7adc2f3e8",
   "metadata": {},
   "source": [
    "At this point, the best way to proceed is to rewrite our thermometer calibration code,\n",
    "this time using autograd, and see what happens. First, we recall our model and loss\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a6dfa57-e94f-4019-b6f3-aa841d764c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w,b):\n",
    "    return (t_u * w + b)\n",
    "def loss_fn(tp, tc):\n",
    "    squarred_diffs = (tp-tc)**2\n",
    "    return squarred_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c9018d4-3a7d-4f84-837f-c467e4659f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4e80f2-305b-4131-9979-b50f36e5258c",
   "metadata": {},
   "source": [
    "**USING THE GRAD ATTRIBUTE**\n",
    "\n",
    "Notice the requires_grad=True argument to the tensor constructor? That argument\n",
    "is telling PyTorch to track the entire family tree of tensors resulting from operations\n",
    "on params. In other words, any tensor that will have params as an ancestor will have\n",
    "access to the chain of functions that were called to get from params to that tensor. In\n",
    "case these functions are differentiable (and most PyTorch tensor operations will be),\n",
    "the value of the derivative will be automatically populated as a grad attribute of the\n",
    "params tensor.\n",
    "\n",
    "In general, all PyTorch tensors have an attribute named grad. Normally, it’s None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1057cd9-2823-425c-b740-e7895122e5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe3570-e27c-4e51-b5a2-3ee7a9ec5940",
   "metadata": {},
   "source": [
    "All we have to do to populate it is to start with a tensor with requires_grad set to\n",
    "True, then call the model and compute the loss, and then call backward on the loss\n",
    "tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e18541a5-0c0a-4009-ade3-996d69dabf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "72c46129-3345-4eed-b0c0-acb63e60dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ae4fa76-1335-4c37-bf42-ad9fca66c89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970e199-235b-4cf4-a661-0920f6ef1126",
   "metadata": {},
   "source": [
    "At this point, the grad attribute of params contains the derivatives of the loss with\n",
    "respect to each element of params.\n",
    "\n",
    "When we compute our loss while the parameters w and b require gradients, in\n",
    "addition to performing the actual computation, PyTorch creates the autograd graph\n",
    "with the operations (in black circles) as nodes, as shown in the top row of figure 5.10. When we call loss.backward(), PyTorch traverses this graph in the reverse\n",
    "direction to compute the gradients, as shown by the arrows in the bottom row of\n",
    "the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330eb8f6-b591-4c39-a2e1-465706c47fce",
   "metadata": {},
   "source": [
    "**ACCUMULATING GRAD FUNCTIONS**\n",
    "\n",
    "We could have any number of tensors with requires_grad set to True and any composition of functions. In this case, PyTorch would compute the derivatives of the loss\n",
    "throughout the chain of functions (the computation graph) and accumulate their values in the grad attribute of those tensors (the leaf nodes of the graph).\n",
    "Alert! Big gotcha ahead. This is something PyTorch newcomers—and a lot of more\n",
    "experienced folks, too—trip up on regularly. We just wrote accumulate, not store.\n",
    "\n",
    "***WARNING*** Calling backward will lead derivatives to accumulate at leaf nodes.\n",
    "We need to zero the gradient explicitly after using it for parameter updates.\n",
    "\n",
    "Let’s repeat together: calling backward will lead derivatives to accumulate at leaf nodes.\n",
    "So if backward was called earlier, the loss is evaluated again, backward is called again\n",
    "(as in any training loop), and the gradient at each leaf is accumulated (that is,\n",
    "summed) on top of the one computed at the previous iteration, which leads to an\n",
    "incorrect value for the gradient.\n",
    "In order to prevent this from occurring, we need to zero the gradient explicitly at each\n",
    "iteration. We can do this easily using the in-place zero_ method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae57ade8-14c1-4ed4-aedc-c9ed15b5ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531974c1-bc3c-4cd9-ae43-f9d50ef31b18",
   "metadata": {},
   "source": [
    "***NOTE*** \n",
    "\n",
    "You might be curious why zeroing the gradient is a required step\n",
    "instead of zeroing happening automatically whenever we call backward.\n",
    "Doing it this way provides more flexibility and control when working with gradients in complicated models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162b6e2-dc86-4ca9-a6cd-d4507b9773a3",
   "metadata": {},
   "source": [
    "Having this reminder drilled into our heads, let’s see what our autograd-enabled\n",
    "training code looks like, start to finish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bf52fe5-ff61-429f-b09c-265d53b384ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= (learning_rate * params)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch %d, Loss %d\" %(epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b073b53-ebbf-4616-8504-3a7bd398da5c",
   "metadata": {},
   "source": [
    "Note that our code updating params is not quite as straightforward as we might have\n",
    "expected. There are two particularities. First, we are encapsulating the update in a\n",
    "no_grad context using the Python with statement. This means within the with block,\n",
    "the PyTorch autograd mechanism should look away: that is, not add edges to the forward graph. In fact, when we are executing this bit of code, the forward graph that\n",
    "PyTorch records is consumed when we call backward, leaving us with the params leaf\n",
    "node. But now we want to change this leaf node before we start building a fresh forward graph on top of it. While this use case is usually wrapped inside the optimizers.\n",
    "\n",
    "Second, we update params in place. This means we keep the same params tensor\n",
    "around but subtract our update from it. When using autograd, we usually avoid inplace updates because PyTorch’s autograd engine might need the values we would be\n",
    "modifying for the backward pass. Here, however, we are operating without autograd,\n",
    "and it is beneficial to keep the params tensor. Not replacing the parameters by assigning new tensors to their variable name will become crucial when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "584863ce-4315-4d72-838b-ec1cc1223a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 186\n",
      "Epoch 1000, Loss 187\n",
      "Epoch 1500, Loss 187\n",
      "Epoch 2000, Loss 187\n",
      "Epoch 2500, Loss 187\n",
      "Epoch 3000, Loss 187\n",
      "Epoch 3500, Loss 187\n",
      "Epoch 4000, Loss 187\n",
      "Epoch 4500, Loss 187\n",
      "Epoch 5000, Loss 187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.4996e-22, 0.0000e+00], requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs=5000, learning_rate=1e-2, params=torch.tensor([1.0, 0.0], requires_grad=True), t_u = t_un, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ee38f-ea82-4a89-b118-c153f45749f6",
   "metadata": {},
   "source": [
    "### Optimizers a la carte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b0b520-1f63-4b20-9bf2-3fa6c2a67368",
   "metadata": {},
   "source": [
    "In the example code, we used vanilla gradient descent for optimization, which worked\n",
    "fine for our simple case. Needless to say, there are several optimization strategies and\n",
    "tricks that can assist convergence, especially when models get complicated.\n",
    "We’ll dive deeper into this topic in later chapters, but now is the right time to\n",
    "introduce the way PyTorch abstracts the optimization strategy away from user code:\n",
    "that is, the training loop we’ve examined. This saves us from the boilerplate busywork\n",
    "of having to update each and every parameter to our model ourselves. The torch\n",
    "module has an optim submodule where we can find classes implementing different\n",
    "optimization algorithms. Here’s an abridged list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2752bf0-1419-42b9-86f6-84647cf92696",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27,\n",
       " ['ASGD',\n",
       "  'Adadelta',\n",
       "  'Adagrad',\n",
       "  'Adam',\n",
       "  'AdamW',\n",
       "  'Adamax',\n",
       "  'LBFGS',\n",
       "  'NAdam',\n",
       "  'Optimizer',\n",
       "  'RAdam',\n",
       "  'RMSprop',\n",
       "  'Rprop',\n",
       "  'SGD',\n",
       "  'SparseAdam',\n",
       "  '__builtins__',\n",
       "  '__cached__',\n",
       "  '__doc__',\n",
       "  '__file__',\n",
       "  '__loader__',\n",
       "  '__name__',\n",
       "  '__package__',\n",
       "  '__path__',\n",
       "  '__spec__',\n",
       "  '_functional',\n",
       "  '_multi_tensor',\n",
       "  'lr_scheduler',\n",
       "  'swa_utils'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dir(optim)), dir(optim), "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d5f9b-d429-4ce0-b658-6272f1ba9a9b",
   "metadata": {},
   "source": [
    "Every optimizer constructor takes a list of parameters (aka PyTorch tensors, typically\n",
    "with requires_grad set to True) as the first input. All parameters passed to the optimizer are retained inside the optimizer object so the optimizer can update their values and access their grad attribute, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f593fbf-f8a8-4c6e-98d9-2eaf8f970ba9",
   "metadata": {},
   "source": [
    "Each optimizer exposes two methods: zero_grad and step. zero_grad zeroes the\n",
    "grad attribute of all the parameters passed to the optimizer upon construction. step\n",
    "updates the value of those parameters according to the optimization strategy implemented by the specific optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35601539-187e-4122-b7b9-b501ed81dc69",
   "metadata": {},
   "source": [
    "**USING A GRADIENT DESCENT OPTIMIZER**\n",
    "\n",
    "Let’s create params and instantiate a gradient descent optimizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e4a9cf0-ba72-483a-a75e-e95f09290423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed490f07-2d2e-4574-93a4-d595413183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046f90f-a45e-4d3f-947f-65f5499492f1",
   "metadata": {},
   "source": [
    "Here SGD stands for stochastic gradient descent. Actually, the optimizer itself is exactly a\n",
    "vanilla gradient descent (as long as the momentum argument is set to 0.0, which is the\n",
    "default). The term stochastic comes from the fact that the gradient is typically obtained\n",
    "by averaging over a random subset of all input samples, called a minibatch. However, the\n",
    "optimizer does not know if the loss was evaluated on all the samples (vanilla) or a random subset of them (stochastic), so the algorithm is literally the same in the two cases.\n",
    "\n",
    "Anyway, let’s take our fancy new optimizer for a spin:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f325defe-5a81-45e5-ac88-c6768ddc714f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "         48.4000, 60.4000, 68.4000], grad_fn=<AddBackward0>),\n",
       " tensor(1763.8848, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "t_p, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2dff6688-d108-428d-8cb7-887c4e8155ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eaf10bf9-1ec8-4577-8628-427b07fe3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4b06707-ed03-48f5-9c1c-b0f0398e2736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef81e52b-c40a-4fa5-94a4-f1d8cb03b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1506b7a9-58ae-45b2-bdc4-957379c04628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e81be8-6116-494b-b5a4-1d1d46553ba0",
   "metadata": {},
   "source": [
    "The value of params is updated upon calling step without us having to touch it ourselves! What happens is that the optimizer looks into params.grad and updates\n",
    "params, subtracting learning_rate times grad from it, exactly as in our former handrolled code.\n",
    "\n",
    "Ready to stick this code in a training loop? Nope! The big gotcha almost got us—\n",
    "we forgot to zero out the gradients. Had we called the previous code in a loop, gradients would have accumulated in the leaves at every call to backward, and our gradient\n",
    "descent would have been all over the place! Here’s the loop-ready code, with the extra\n",
    "zero_grad at the correct spot (right before the call to backward):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a61d1e2c-e66d-4f7c-abdf-3bd6f271d2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e3e5b-dc4c-46a2-b284-d244f66fdd74",
   "metadata": {},
   "source": [
    "Perfect! See how the optim module helps us abstract away the specific optimization\n",
    "scheme? All we have to do is provide a list of params to it (that list can be extremely long, as is needed for very deep neural network models), and we can forget about the details.\n",
    "\n",
    "Let’s update our training loop accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e83082d-83d0-489c-a955-0e4024033232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%500 == 0:\n",
    "            print(\"Epoch %d, Loss %f\" % (epoch, loss))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a131deab-8ca7-470d-a6d1-9d0e40271a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "optimizer = optim.SGD({params}, lr = 1e-2)\n",
    "params = training_loop(5000,  optimizer=optimizer, params=params, t_u=t_un, t_c=t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ba0135b-c9aa-4bdc-b2f4-82feb52705c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d79112-f363-4f06-a61b-2c0d43352f65",
   "metadata": {},
   "source": [
    "Again, we get the same result as before. Great: this is further confirmation that we\n",
    "know how to descend a gradient by hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dbc6a-d5a0-4b21-8603-24ec5c9dc71f",
   "metadata": {},
   "source": [
    "### Testing Other Optimizers ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc8c36-e555-4947-98ba-044388dfd312",
   "metadata": {},
   "source": [
    "In order to test more optimizers, all we have to do is instantiate a different optimizer,\n",
    "say Adam, instead of SGD. The rest of the code stays as it is. Pretty handy stuff.\n",
    "We won’t go into much detail about Adam; suffice to say that it is a more sophisticated optimizer in which the learning rate is set adaptively. In addition, it is a lot less\n",
    "sensitive to the scaling of the parameters—so insensitive that we can go back to using the original (non-normalized) input t_u, and even increase the learning rate to 1e-1,\n",
    "and Adam won’t even blink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ebefad1-dd17-411a-b2ef-e8d858b2a982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_r = t_u.clone().detach()\n",
    "t_r.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44a17bcc-421c-4ef2-8494-dc510bbe2765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612900\n",
      "Epoch 1000, Loss 3.086700\n",
      "Epoch 1500, Loss 2.928579\n",
      "Epoch 2000, Loss 2.927644\n",
      "Epoch 2500, Loss 2.927645\n",
      "Epoch 3000, Loss 2.927646\n",
      "Epoch 3500, Loss 2.927645\n",
      "Epoch 4000, Loss 2.927646\n",
      "Epoch 4500, Loss 2.927646\n",
      "Epoch 5000, Loss 2.927645\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad = True)\n",
    "optimizer = optim.Adam([params], lr = 1e-1)\n",
    "params = training_loop(n_epochs=5000, optimizer=optimizer, params=params, t_u=t_r, t_c=t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7defd071-c6ce-4bcf-9379-2904f2b4f0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.5368, -17.3048], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6064b4-bb41-4081-932f-253ea4b4b1b0",
   "metadata": {},
   "source": [
    "The optimizer is not the only flexible part of our training loop. Let’s turn our attention to the model. In order to train a neural network on the same data and the same\n",
    "loss, all we would need to change is the model function. It wouldn’t make particular\n",
    "sense in this case, since we know that converting Celsius to Fahrenheit amounts to a\n",
    "linear transformation. We’ll see quite soon that\n",
    "neural networks allow us to remove our arbitrary assumptions about the shape of the\n",
    "function we should be approximating. Even so, we’ll see how neural networks manage\n",
    "to be trained even when the underlying processes are highly nonlinear.\n",
    "\n",
    "We have touched on a lot of the essential concepts that will enable us to train\n",
    "complicated deep learning models while knowing what’s going on under the hood:\n",
    "backpropagation to estimate gradients, autograd, and optimizing weights of models\n",
    "using gradient descent or other optimizers. Really, there isn’t a lot more. The rest is\n",
    "mostly filling in the blanks, however extensive they are.\n",
    "\n",
    "Next up, we’re going to offer an aside on how to split our samples, because that\n",
    "sets up a perfect use case for learning how to better control autograd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f06cb2-4fc0-46e7-8a61-29f48cf7190a",
   "metadata": {},
   "source": [
    "### Training, validation, and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193bf49-d898-4181-a5eb-087c2d28c8cf",
   "metadata": {},
   "source": [
    "Johannes Kepler taught us one last thing that we didn’t discuss so far, remember? He\n",
    "kept part of the data on the side so that he could validate his models on independent\n",
    "observations. This is a vital thing to do, especially when the model we adopt could\n",
    "potentially approximate functions of any shape, as in the case of neural networks. In\n",
    "other words, a highly adaptable model will tend to use its many parameters to make\n",
    "sure the loss is minimal at the data points, but we’ll have no guarantee that the model\n",
    "behaves well away from or in between the data points. After all, that’s what we’re asking\n",
    "the optimizer to do: minimize the loss at the data points. Sure enough, if we had independent data points that we didn’t use to evaluate our loss or descend along its negative gradient, we would soon find out that evaluating the loss at those independent\n",
    "data points would yield higher-than-expected loss. We have already mentioned this\n",
    "phenomenon, called overfitting.\n",
    "\n",
    "The first action we can take to combat overfitting is recognizing that it might happen. In order to do so, as Kepler figured out in 1600, we must take a few data points out of our dataset (the validation set) and only fit our model on the remaining data\n",
    "points (the training set). Then, while we’re fitting the model,\n",
    "we can evaluate the loss once on the training set and once on the validation set. When\n",
    "we’re trying to decide if we’ve done a good job of fitting our model to the data, we\n",
    "must look at both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7feb6c-9594-4a10-a269-0a5d8e358d77",
   "metadata": {},
   "source": [
    "**EVALUATING THE TRAINING LOSS**\n",
    "\n",
    "The training loss will tell us if our model can fit the training set at all—in other words,\n",
    "if our model has enough capacity to process the relevant information in the data. If\n",
    "our mysterious thermometer somehow managed to measure temperatures using a logarithmic scale, our poor linear model would not have had a chance to fit those measurements and provide us with a sensible conversion to Celsius. In that case, our\n",
    "training loss (the loss we were printing in the training loop) would stop decreasing\n",
    "well before approaching zero.\n",
    "\n",
    "A deep neural network can potentially approximate complicated functions, provided that the number of neurons, and therefore parameters, is high enough. The\n",
    "fewer the number of parameters, the simpler the shape of the function our network will\n",
    "be able to approximate. So, rule 1: if the training loss is not decreasing, chances are the\n",
    "model is too simple for the data. The other possibility is that our data just doesn’t contain meaningful information that lets it explain the output: if the nice folks at the shop\n",
    "sell us a barometer instead of a thermometer, we will have little chance of predicting\n",
    "temperature in Celsius from just pressure, even if we use the latest neural network\n",
    "architecture from Quebec (www.umontreal.ca/en/artificialintelligence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91362e5-c144-467c-898e-389f2b0ac60a",
   "metadata": {},
   "source": [
    "What about the validation set? Well, if the loss evaluated in the validation set doesn’t\n",
    "decrease along with the training set, it means our model is improving its fit of the samples it is seeing during training, but it is not generalizing to samples outside this precise\n",
    "set. As soon as we evaluate the model at new, previously unseen points, the values of\n",
    "the loss function are poor. So, rule 2: if the training loss and the validation loss\n",
    "diverge, we’re overfitting.\n",
    "\n",
    "Let’s delve into this phenomenon a little, going back to our thermometer example. We could have decided to fit the data with a more complicated function, like a\n",
    "piecewise polynomial or a really large neural network. It could generate a model\n",
    "meandering its way through the data points, just because it pushes\n",
    "the loss very close to zero. Since the behavior of the function away from the data\n",
    "points does not increase the loss, there’s nothing to keep the model in check for\n",
    "inputs away from the training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f091518-aa22-4a9e-a81a-a6839a1b5f59",
   "metadata": {},
   "source": [
    "What’s the cure, though? Good question. From what we just said, overfitting really\n",
    "looks like a problem of making sure the behavior of the model in between data points\n",
    "is sensible for the process we’re trying to approximate. First of all, we should make\n",
    "sure we get enough data for the process. If we collected data from a sinusoidal process by sampling it regularly at a low frequency, we would have a hard time fitting a\n",
    "model to it.\n",
    "\n",
    "Assuming we have enough data points, we should make sure the model that is\n",
    "capable of fitting the training data is as regular as possible in between them. There are\n",
    "several ways to achieve this. One is adding penalization terms to the loss function, to\n",
    "make it cheaper for the model to behave more smoothly and change more slowly (up\n",
    "to a point). Another is to add noise to the input samples, to artificially create new data\n",
    "points in between training data samples and force the model to try to fit those, too.\n",
    "There are several other ways, all of them somewhat related to these. But the best favor\n",
    "we can do to ourselves, at least as a first move, is to make our model simpler. From an\n",
    "intuitive standpoint, a simpler model may not fit the training data as perfectly as a\n",
    "more complicated model would, but it will likely behave more regularly in between\n",
    "data points.\n",
    "\n",
    "We’ve got some nice trade-offs here. On the one hand, we need the model to have\n",
    "enough capacity for it to fit the training set. On the other, we need the model to avoid\n",
    "overfitting. Therefore, in order to choose the right size for a neural network model in\n",
    "terms of parameters, the process is based on two steps: increase the size until it fits,\n",
    "and then scale it down until it stops overfitting.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d401364-9925-46c6-ae73-fe02b00a04e0",
   "metadata": {},
   "source": [
    "We’ll see more about this soon — we’ll discover that our life will be a balancing act between fitting and overfitting. For now, let’s get back to our example and\n",
    "see how we can split the data into a training set and a validation set. We’ll do it by\n",
    "shuffling t_u and t_c the same way and then splitting the resulting shuffled tensors\n",
    "into two parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d305044-cc9e-4c8d-9765-ea2035943ce4",
   "metadata": {},
   "source": [
    "**SPLITTING A DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6f78be5-9a9d-4ff4-a48d-1fe5d0ac1d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_smaples = t_u.shape[0]\n",
    "n_smaples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce619660-385a-400c-9da0-3d98ff2ad5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val = int(0.2 * n_smaples)\n",
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e638256-204f-4f6a-b557-8682c14bed43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  9,  7,  0,  3,  4,  2,  8,  6, 10,  5])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices = torch.randperm(n_smaples)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9cbd7218-46a0-455d-a49a-d0f2bfe38277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  9,  7,  0,  3,  4,  2,  8,  6, 10,  5])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_indices[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57ea9a26-7cca-4fd9-b5b5-0d3b0f948676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 7, 0, 3, 4, 2, 8, 6])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices = (shuffled_indices[:-n_val])\n",
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8dde91b-7d97-4064-bec7-8355e7af3cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  5])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_indices = shuffled_indices[-n_val:]\n",
    "val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30cb09c6-8238-471a-8fe4-ad523ef8c053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 9, 7, 0, 3, 4, 2, 8, 6]), tensor([10,  5]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b39adc-1b55-4cc1-9dde-0dcf0b48e56b",
   "metadata": {},
   "source": [
    "We just got index tensors that we can use to build training and validation sets starting\n",
    "from the data tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5ae0aed-2731-472e-9acb-732f5a30d9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([55.9000, 60.4000, 21.8000, 35.7000, 81.9000, 56.3000, 58.2000, 48.4000,\n",
       "         33.9000]),\n",
       " tensor([14.0000, 13.0000, -4.0000,  0.5000, 28.0000, 11.0000, 15.0000,  6.0000,\n",
       "          3.0000]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "train_t_u, train_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d937a53-2eaf-4da3-bbce-9e0f7614e454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([68.4000, 48.9000]), tensor([21.,  8.]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "val_t_u, val_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ae05961-48eb-495b-b0f1-22dfb6fded9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.5900, 6.0400, 2.1800, 3.5700, 8.1900, 5.6300, 5.8200, 4.8400, 3.3900]),\n",
       " tensor([6.8400, 4.8900]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_t_un = train_t_u * 0.1\n",
    "val_t_un = val_t_u * 0.1\n",
    "train_t_un, val_t_un"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb798dd-39cb-4788-a859-e8bba62fbc75",
   "metadata": {},
   "source": [
    "Our training loop doesn’t really change. We just want to additionally evaluate the validation loss at every epoch, to have a chance to recognize whether we’re overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bedab09b-4f38-4f21-9058-91cb87c2fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss =loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch%500 == 0:\n",
    "            print(\"Epoch %d, Training Loss %.3f, Validation Loss %.3f\" % (epoch, train_loss, val_loss))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "179d4f2a-0d3c-4288-a577-a4048f0e07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_un = train_t_un.clone().detach()\n",
    "val_t_un = val_t_un.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5544fb5-f4e7-4ff6-9b12-2ee7e1788da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Training Loss 7.321, Validation Loss 7.020\n",
      "Epoch 1000, Training Loss 3.753, Validation Loss 3.671\n",
      "Epoch 1500, Training Loss 3.240, Validation Loss 2.683\n",
      "Epoch 2000, Training Loss 3.166, Validation Loss 2.349\n",
      "Epoch 2500, Training Loss 3.155, Validation Loss 2.228\n",
      "Epoch 3000, Training Loss 3.154, Validation Loss 2.183\n",
      "Epoch 3500, Training Loss 3.154, Validation Loss 2.166\n",
      "Epoch 4000, Training Loss 3.154, Validation Loss 2.160\n",
      "Epoch 4500, Training Loss 3.154, Validation Loss 2.158\n",
      "Epoch 5000, Training Loss 3.154, Validation Loss 2.157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.2484, -16.7768], requires_grad=True)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "optimizer = optim.SGD([params], lr=1e-2)\n",
    "params = training_loop(n_epochs=5000, optimizer=optimizer, params=params, train_t_u=train_t_un, train_t_c=train_t_c, val_t_u=val_t_un, \n",
    "                       val_t_c=val_t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb0e6dd-ad76-4750-97e2-fbe74314f459",
   "metadata": {},
   "source": [
    "Here we are not being entirely fair to our model. The validation set is really small, so\n",
    "the validation loss will only be meaningful up to a point. In any case, we note that the\n",
    "validation loss is higher than our training loss, although not by an order of magnitude. We expect a model to perform better on the training set, since the model\n",
    "parameters are being shaped by the training set. Our main goal is to also see both the\n",
    "training loss and the validation loss decreasing. While ideally both losses would be\n",
    "roughly the same value, as long as the validation loss stays reasonably close to the\n",
    "training loss, we know that our model is continuing to learn generalized things about\n",
    "our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026bd01e-23aa-47d8-8483-50eb4212c8fe",
   "metadata": {},
   "source": [
    "### Autograd nits and switching it off\n",
    "From the previous training loop, we can appreciate that we only ever call backward on\n",
    "train_loss. Therefore, errors will only ever backpropagate based on the training\n",
    "set—the validation set is used to provide an independent evaluation of the accuracy of\n",
    "the model’s output on data that wasn’t used for training.\n",
    "\n",
    "The curious reader will have an embryo of a question at this point. The model is\n",
    "evaluated twice—once on train_t_u and once on val_t_u—and then backward is\n",
    "called. Won’t this confuse autograd? Won’t backward be influenced by the values generated during the pass on the validation set?\n",
    "\n",
    "Luckily for us, this isn’t the case. The first line in the training loop evaluates model\n",
    "on train_t_u to produce train_t_p. Then train_loss is evaluated from train_t_p.\n",
    "This creates a computation graph that links train_t_u to train_t_p to train_loss.\n",
    "When model is evaluated again on val_t_u, it produces val_t_p and val_loss. In this\n",
    "case, a separate computation graph will be created that links val_t_u to val_t_p to\n",
    "val_loss. Separate tensors have been run through the same functions, model and\n",
    "loss_fn, generating separate computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8470a7-62da-4c74-b83b-674625ca25fb",
   "metadata": {},
   "source": [
    "The only tensors these two graphs have in common are the parameters. When we call\n",
    "backward on train_loss, we run backward on the first graph. In other words, we\n",
    "accumulate the derivatives of train_loss with respect to the parameters based on the\n",
    "computation generated from train_t_u.\n",
    "\n",
    "If we (incorrectly) called backward on val_loss as well, we would accumulate the\n",
    "derivatives of val_loss with respect to the parameters on the same leaf nodes. Remember\n",
    "the zero_grad thing, whereby gradients are accumulated on top of each other every\n",
    "time we call backward unless we zero out the gradients explicitly? Well, here something very similar would happen: calling backward on val_loss would lead to gradients accumulating in the params tensor, on top of those generated during the train_loss.backward() call. In this case, we would effectively train our model on the whole dataset (both\n",
    "training and validation), since the gradient would depend on both. Pretty interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231de8e7-9cb9-421d-97f0-66de37a07c55",
   "metadata": {},
   "source": [
    "There’s another element for discussion here. Since we’re not ever calling backward on val_loss, why are we building the graph in the first place? We could in fact\n",
    "just call model and loss_fn as plain functions, without tracking the computation.\n",
    "However optimized, building the autograd graph comes with additional costs that we\n",
    "could totally forgo during the validation pass, especially when the model has millions\n",
    "of parameters.\n",
    "\n",
    "In order to address this, PyTorch allows us to switch off autograd when we don’t\n",
    "need it, using the torch.no_grad context manager.12 We won’t see any meaningful\n",
    "advantage in terms of speed or memory consumption on our small problem. However, for larger models, the differences can add up. We can make sure this works by\n",
    "checking the value of the requires_grad attribute on the val_loss tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f238c5d-3af9-485e-8ba0-7e9e10e28a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, val_t_c)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch %500 == 0:\n",
    "            print(\"Epoch %d, Trainling Loss %.3f, Validation Loss %.3f\" % (epcoh, train_loss, val_loss))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb9bcd-adde-4cb2-96a4-ed98ae05c167",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "92084b88-4362-43fa-883d-a3aa0a757536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w1, w2, b):\n",
    "    return (w2 * t_u ** 2 + w1 * t_u + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4656d70f-d483-456d-9d95-e6967e6d12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, train_t_c, val_t_u, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch %500 == 0:\n",
    "            print(\"Epoch %d, Trainling Loss %.3f, Validation Loss %.3f\" % (epoch, train_loss, val_loss))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d6ff48d-7d20-487a-8b6f-20475f182472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67a2087a-9ba1-4e3a-aaed-87e766bfb9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Trainling Loss 6.402, Validation Loss 4.132\n",
      "Epoch 1000, Trainling Loss 4.071, Validation Loss 2.834\n",
      "Epoch 1500, Trainling Loss 3.337, Validation Loss 2.300\n",
      "Epoch 2000, Trainling Loss 3.188, Validation Loss 2.137\n",
      "Epoch 2500, Trainling Loss 3.119, Validation Loss 2.081\n",
      "Epoch 3000, Trainling Loss 3.043, Validation Loss 2.048\n",
      "Epoch 3500, Trainling Loss 2.955, Validation Loss 2.014\n",
      "Epoch 4000, Trainling Loss 2.855, Validation Loss 1.974\n",
      "Epoch 4500, Trainling Loss 2.747, Validation Loss 1.930\n",
      "Epoch 5000, Trainling Loss 2.637, Validation Loss 1.883\n",
      "Epoch 5500, Trainling Loss 2.530, Validation Loss 1.834\n",
      "Epoch 6000, Trainling Loss 2.436, Validation Loss 1.787\n",
      "Epoch 6500, Trainling Loss 2.361, Validation Loss 1.744\n",
      "Epoch 7000, Trainling Loss 2.309, Validation Loss 1.708\n",
      "Epoch 7500, Trainling Loss 2.278, Validation Loss 1.680\n",
      "Epoch 8000, Trainling Loss 2.264, Validation Loss 1.660\n",
      "Epoch 8500, Trainling Loss 2.259, Validation Loss 1.648\n",
      "Epoch 9000, Trainling Loss 2.257, Validation Loss 1.642\n",
      "Epoch 9500, Trainling Loss 2.257, Validation Loss 1.639\n",
      "Epoch 10000, Trainling Loss 2.257, Validation Loss 1.638\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam([params], lr=1e-2)\n",
    "params = training_loop(n_epochs=10000, optimizer=optimizer, params=params, train_t_u=train_t_un,train_t_c=train_t_c, val_t_u=val_t_un, \n",
    "                       val_t_c=val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18ed23-b188-490c-b488-52eeae6cfb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
