{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7dcc22-8b96-45dc-866e-baa945d73d29",
   "metadata": {},
   "source": [
    "# Real-world data representation using tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf83354-a3fc-44b0-8a68-3bdefbfca0be",
   "metadata": {},
   "source": [
    "### following things will be covered:\n",
    "- Representing real-world data as PyTorch tensors\n",
    "- Working with a range of data types\n",
    "- Loading data from a file\n",
    "- Converting data to tensors\n",
    "- Shaping tensors so they can be used as inputs for neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b564c-aada-4855-b3e8-bc57f215e5e7",
   "metadata": {},
   "source": [
    "Neural networks take tensors as input and produce tensors as outputs. In\n",
    "fact, all operations within a neural network and during optimization are operations\n",
    "between tensors, and all parameters (for example, weights and biases) in a neural\n",
    "network are tensors. Having a good sense of how to perform operations on tensors\n",
    "and index them effectively is central to using tools like PyTorch successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d29ca-fb5f-470c-b1c7-e394648aee19",
   "metadata": {},
   "source": [
    "## Working with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554dc5e8-f103-4afa-8ca2-b5ee57558c3f",
   "metadata": {},
   "source": [
    "An image is represented as a collection of scalars arranged in a regular grid with a\n",
    "height and a width (in pixels). We might have a single scalar per grid point (the\n",
    "pixel), which would be represented as a grayscale image; or multiple scalars per grid\n",
    "point, which would typically represent different colors, as we saw in the previous chapter, or different features like depth from a depth camera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62a57f-8b75-4cc5-afde-c02850c08e1a",
   "metadata": {},
   "source": [
    "### Adding color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3aabe-4c17-4fa9-abef-8a19e75dc2e0",
   "metadata": {},
   "source": [
    "RGB is defined by three numbers representing the intensity of red, green, and blue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c48a3-4525-4e7e-85bc-32bc348ee0dc",
   "metadata": {},
   "source": [
    "### Loading an image file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a3636-6342-40df-852d-4d66f5106be4",
   "metadata": {},
   "source": [
    "Images come in several different file formats, but luckily there are plenty of ways to\n",
    "load images in Python. Let’s start by loading a PNG image using the imageio module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ca326d-e315-45c4-9b6d-560745084497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.adil\\Miniconda3\\envs\\sahi\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb7863d-fc2d-4604-8259-559892524a50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.adil\\AppData\\Local\\Temp\\ipykernel_2520\\4178578973.py:1: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img_arr = imageio.imread('./dlwpt-code-master/dlwpt-code-master/data/p1ch4/image-dog/bobby.jpg')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[77, 45, 22],\n",
       "        [77, 45, 22],\n",
       "        [78, 46, 21]],\n",
       "\n",
       "       [[75, 43, 20],\n",
       "        [76, 44, 21],\n",
       "        [77, 45, 20]],\n",
       "\n",
       "       [[74, 39, 17],\n",
       "        [75, 41, 16],\n",
       "        [77, 43, 18]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr = imageio.imread('./dlwpt-code-master/dlwpt-code-master/data/p1ch4/image-dog/bobby.jpg')\n",
    "img_arr[:3, :3, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc9716-445b-48f1-86ab-9911ba4fa67a",
   "metadata": {},
   "source": [
    "### Changing the layout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459d580-ca2e-43a4-8f12-cf1971a64b65",
   "metadata": {},
   "source": [
    "We can use the tensor’s permute method with the old dimensions for each new dimension to get to an appropriate layout. Given an input tensor H × W × C as obtained previously, we get a proper layout by having channel 2 first and then channels 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94bddd52-eb06-43a4-91ef-159c2690d5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 720, 1280]),\n",
       " tensor([[[ 77,  77,  78,  ..., 118, 117, 116],\n",
       "          [ 75,  76,  77,  ..., 118, 117, 116],\n",
       "          [ 74,  75,  77,  ..., 119, 117, 116],\n",
       "          ...,\n",
       "          [215, 216, 217,  ..., 172, 174, 174],\n",
       "          [215, 216, 217,  ..., 173, 174, 174],\n",
       "          [215, 216, 217,  ..., 159, 158, 158]]], dtype=torch.uint8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.from_numpy(img_arr)\n",
    "out = img.permute(2, 0 ,1)\n",
    "out.shape, out[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d3272-a65c-43f1-93cc-cb6f38824f8f",
   "metadata": {},
   "source": [
    "This operation does not make a copy of the\n",
    "tensor data. Instead, out uses the same underlying storage as img and only plays with\n",
    "the size and stride information at the tensor level. This is convenient because the\n",
    "operation is very cheap; but just as a heads-up: changing a pixel in img will lead to a\n",
    "change in out.Note also that other deep learning frameworks use different layouts. For instance,\n",
    "originally TensorFlow kept the channel dimension last, resulting in an H × W × C layout (it now supports multiple layouts). This strategy has pros and cons from a low-level\n",
    "performance standpoint, but for our concerns, it doesn’t make a difference as long as\n",
    "we reshape our tensors properly.Note also that other deep learning frameworks use different layouts. For instance,\n",
    "originally TensorFlow kept the channel dimension last, resulting in an H × W × C layout (it now supports multiple layouts). This strategy has pros and cons from a low-level\n",
    "performance standpoint, but for our concerns, it doesn’t make a difference as long as\n",
    "we reshape our tensors properly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae3036-d020-4694-838d-38b05332d485",
   "metadata": {},
   "source": [
    "So far, we have described a single image. Following the same strategy we’ve used\n",
    "for earlier data types, to create a dataset of multiple images to use as an input for our\n",
    "neural networks, we store the images in a batch along the first dimension to obtain an\n",
    "N × C × H × W tensor.\n",
    "\n",
    "As a slightly more efficient alternative to using stack to build up the tensor, we can preallocate a tensor of appropriate size and fill it with images loaded from a directory, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1b85fd-ccd1-4f1d-b33c-90c4766126c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "batch = torch.zeros(batch_size, 3, 256, 256).to(torch.uint8)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf0875-ac60-40f7-8aa8-c1008528689d",
   "metadata": {},
   "source": [
    "We can now load all PNG images from an input directory and store them in\n",
    "the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc71a6f-a9a6-4546-b8b6-6e362bac56e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat1.png', 'cat2.png', 'cat3.png']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './dlwpt-code-master/dlwpt-code-master/data/p1ch4/image-cats/'\n",
    "filename = [name for name in os.listdir(data_dir) if os.path.splitext(name)[1]=='.png']\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85288c7-7899-4db7-8686-db76b46ddb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 256, 256])\n",
      "torch.Size([3, 3, 256, 256])\n",
      "torch.Size([3, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.adil\\AppData\\Local\\Temp\\ipykernel_2520\\2960609711.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img_arr = imageio.imread(os.path.join(data_dir, file))\n"
     ]
    }
   ],
   "source": [
    "for i, file in enumerate(filename):\n",
    "    img_arr = imageio.imread(os.path.join(data_dir, file))\n",
    "    img_t = torch.from_numpy(img_arr)\n",
    "    img_t = img_t.permute(2,0,1)\n",
    "    img_t = img_t[:3]\n",
    "    batch[i] = img_t\n",
    "    print(batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22291c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80af567-59d3-4e11-a27c-f8640fcafa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f6fbc-6dee-485b-8751-a6d7ddc9a74e",
   "metadata": {},
   "source": [
    "### Normalizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176d9f36-b1a2-41a9-a997-9ffcae35352c",
   "metadata": {},
   "source": [
    "Neural networks usually work with floating-point tensors as\n",
    "their input. Neural networks exhibit the best training performance when the input\n",
    "data ranges roughly from 0 to 1, or from -1 to 1 (this is an effect of how their building\n",
    "blocks are defined).\n",
    "\n",
    "So a typical thing we’ll want to do is cast a tensor to floating-point and normalize\n",
    "the values of the pixels. Casting to floating-point is easy, but normalization is trickier,\n",
    "as it depends on what range of the input we decide should lie between 0 and 1 (or -1\n",
    "and 1). One possibility is to just divide the values of the pixels by 255 (the maximum\n",
    "representable number in 8-bit unsigned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a264f3-8ae2-458b-9722-5e6d27da8e49",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6118, 0.5961, 0.4863,  ..., 0.5882, 0.5843, 0.6196],\n",
       "          [0.6824, 0.5255, 0.6471,  ..., 0.4706, 0.5333, 0.5412],\n",
       "          [0.4980, 0.6118, 0.4196,  ..., 0.5137, 0.5608, 0.6431],\n",
       "          ...,\n",
       "          [0.4549, 0.5098, 0.5059,  ..., 0.4980, 0.4627, 0.4392],\n",
       "          [0.5059, 0.5098, 0.4824,  ..., 0.4510, 0.4745, 0.4471],\n",
       "          [0.5059, 0.4824, 0.4627,  ..., 0.4431, 0.4745, 0.4706]],\n",
       "\n",
       "         [[0.5451, 0.5294, 0.4275,  ..., 0.5294, 0.5294, 0.5765],\n",
       "          [0.6275, 0.4667, 0.5843,  ..., 0.4118, 0.4784, 0.4863],\n",
       "          [0.4431, 0.5490, 0.3529,  ..., 0.4627, 0.5059, 0.5961],\n",
       "          ...,\n",
       "          [0.3882, 0.4314, 0.4353,  ..., 0.4588, 0.4235, 0.4039],\n",
       "          [0.4353, 0.4353, 0.4157,  ..., 0.4157, 0.4392, 0.4118],\n",
       "          [0.4353, 0.4078, 0.4000,  ..., 0.4039, 0.4314, 0.4353]],\n",
       "\n",
       "         [[0.5059, 0.4824, 0.3843,  ..., 0.5137, 0.5176, 0.5686],\n",
       "          [0.6078, 0.4314, 0.5373,  ..., 0.4000, 0.4667, 0.4745],\n",
       "          [0.4078, 0.5176, 0.3137,  ..., 0.4392, 0.4902, 0.5725],\n",
       "          ...,\n",
       "          [0.3647, 0.4235, 0.4118,  ..., 0.4902, 0.4510, 0.4235],\n",
       "          [0.4235, 0.4235, 0.3843,  ..., 0.4314, 0.4588, 0.4314],\n",
       "          [0.4196, 0.3843, 0.3725,  ..., 0.4235, 0.4510, 0.4549]]],\n",
       "\n",
       "\n",
       "        [[[0.7922, 0.7569, 0.7451,  ..., 0.0510, 0.0510, 0.0471],\n",
       "          [0.7804, 0.7529, 0.7412,  ..., 0.0549, 0.0549, 0.0549],\n",
       "          [0.7765, 0.7569, 0.7373,  ..., 0.0471, 0.0471, 0.0471],\n",
       "          ...,\n",
       "          [0.3647, 0.3216, 0.2980,  ..., 0.1412, 0.1412, 0.1412],\n",
       "          [0.2941, 0.2667, 0.3961,  ..., 0.1412, 0.1412, 0.1451],\n",
       "          [0.3333, 0.4039, 0.3529,  ..., 0.1412, 0.1451, 0.1490]],\n",
       "\n",
       "         [[0.5922, 0.5451, 0.5216,  ..., 0.0353, 0.0353, 0.0314],\n",
       "          [0.5922, 0.5490, 0.5255,  ..., 0.0431, 0.0431, 0.0431],\n",
       "          [0.5961, 0.5608, 0.5255,  ..., 0.0431, 0.0431, 0.0431],\n",
       "          ...,\n",
       "          [0.2235, 0.1765, 0.1529,  ..., 0.1020, 0.1020, 0.1020],\n",
       "          [0.1294, 0.1020, 0.2314,  ..., 0.1020, 0.1020, 0.1059],\n",
       "          [0.1569, 0.2275, 0.1765,  ..., 0.1020, 0.1059, 0.1098]],\n",
       "\n",
       "         [[0.2667, 0.2078, 0.1725,  ..., 0.0235, 0.0235, 0.0196],\n",
       "          [0.2627, 0.2118, 0.1725,  ..., 0.0235, 0.0235, 0.0235],\n",
       "          [0.2627, 0.2196, 0.1725,  ..., 0.0235, 0.0235, 0.0235],\n",
       "          ...,\n",
       "          [0.1216, 0.0745, 0.0471,  ..., 0.0667, 0.0667, 0.0667],\n",
       "          [0.0431, 0.0078, 0.1373,  ..., 0.0667, 0.0667, 0.0706],\n",
       "          [0.0745, 0.1451, 0.0863,  ..., 0.0667, 0.0706, 0.0745]]],\n",
       "\n",
       "\n",
       "        [[[0.9333, 0.9333, 0.9333,  ..., 0.8392, 0.8431, 0.8431],\n",
       "          [0.9333, 0.9333, 0.9333,  ..., 0.8392, 0.8431, 0.8431],\n",
       "          [0.9333, 0.9333, 0.9333,  ..., 0.8392, 0.8431, 0.8431],\n",
       "          ...,\n",
       "          [0.8392, 0.8353, 0.8314,  ..., 0.7333, 0.7451, 0.7569],\n",
       "          [0.8392, 0.8353, 0.8314,  ..., 0.7294, 0.7451, 0.7529],\n",
       "          [0.8392, 0.8353, 0.8314,  ..., 0.7294, 0.7451, 0.7529]],\n",
       "\n",
       "         [[0.7647, 0.7647, 0.7647,  ..., 0.6784, 0.6863, 0.6863],\n",
       "          [0.7647, 0.7647, 0.7647,  ..., 0.6784, 0.6863, 0.6863],\n",
       "          [0.7647, 0.7647, 0.7647,  ..., 0.6784, 0.6863, 0.6863],\n",
       "          ...,\n",
       "          [0.5020, 0.4980, 0.4941,  ..., 0.3922, 0.4039, 0.4157],\n",
       "          [0.5020, 0.4980, 0.4941,  ..., 0.3882, 0.4039, 0.4118],\n",
       "          [0.5020, 0.4980, 0.4941,  ..., 0.3882, 0.4039, 0.4118]],\n",
       "\n",
       "         [[0.5373, 0.5373, 0.5373,  ..., 0.4902, 0.4941, 0.4941],\n",
       "          [0.5373, 0.5373, 0.5373,  ..., 0.4902, 0.4941, 0.4941],\n",
       "          [0.5373, 0.5373, 0.5373,  ..., 0.4902, 0.4941, 0.4941],\n",
       "          ...,\n",
       "          [0.3098, 0.3059, 0.3020,  ..., 0.2510, 0.2667, 0.2824],\n",
       "          [0.3098, 0.3059, 0.3020,  ..., 0.2510, 0.2706, 0.2784],\n",
       "          [0.3098, 0.3059, 0.3020,  ..., 0.2549, 0.2706, 0.2824]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = batch.float()\n",
    "batch = batch/255\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29717e34-31f1-45d7-b722-af1fc030ca6a",
   "metadata": {},
   "source": [
    "Another possibility is to compute the mean and standard deviation of the input data\n",
    "and scale it so that the output has zero mean and unit standard deviation across each\n",
    "channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224a8e21-18c2-4b4c-82c3-05b1e3818bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels = batch.shape[1]\n",
    "n_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24ef4a4-dcac-494e-a18b-c5b8dfec290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(n_channels):\n",
    "    mean = torch.mean(batch[:, c])\n",
    "    std = torch.std(batch[:, c])\n",
    "    batch[:, c] = (batch[:, c] - mean)/std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86025a30-3bbd-4786-b201-4c7c1545852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:, 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86854210-ad5e-45a3-a9bd-fe3f1c104099",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1439,  0.0730, -0.4234,  ...,  0.0375,  0.0198,  0.1794],\n",
       "          [ 0.4631, -0.2461,  0.3035,  ..., -0.4944, -0.2107, -0.1752],\n",
       "          [-0.3703,  0.1439, -0.7249,  ..., -0.2993, -0.0866,  0.2858],\n",
       "          ...,\n",
       "          [-0.5653, -0.3171, -0.3348,  ..., -0.3703, -0.5298, -0.6362],\n",
       "          [-0.3348, -0.3171, -0.4412,  ..., -0.5830, -0.4766, -0.6007],\n",
       "          [-0.3348, -0.4412, -0.5298,  ..., -0.6185, -0.4766, -0.4944]],\n",
       "\n",
       "         [[ 0.4632,  0.3874, -0.1058,  ...,  0.3874,  0.3874,  0.6150],\n",
       "          [ 0.8615,  0.0839,  0.6529,  ..., -0.1816,  0.1408,  0.1787],\n",
       "          [-0.0299,  0.4822, -0.4661,  ...,  0.0649,  0.2736,  0.7098],\n",
       "          ...,\n",
       "          [-0.2954, -0.0868, -0.0678,  ...,  0.0460, -0.1247, -0.2196],\n",
       "          [-0.0678, -0.0678, -0.1627,  ..., -0.1627, -0.0489, -0.1816],\n",
       "          [-0.0678, -0.2006, -0.2385,  ..., -0.2196, -0.0868, -0.0678]],\n",
       "\n",
       "         [[ 0.7792,  0.6573,  0.1495,  ...,  0.8198,  0.8401,  1.1041],\n",
       "          [ 1.3072,  0.3933,  0.9417,  ...,  0.2308,  0.5761,  0.6167],\n",
       "          [ 0.2714,  0.8401, -0.2161,  ...,  0.4339,  0.6979,  1.1245],\n",
       "          ...,\n",
       "          [ 0.0480,  0.3526,  0.2917,  ...,  0.6979,  0.4948,  0.3526],\n",
       "          [ 0.3526,  0.3526,  0.1495,  ...,  0.3933,  0.5354,  0.3933],\n",
       "          [ 0.3323,  0.1495,  0.0886,  ...,  0.3526,  0.4948,  0.5151]]],\n",
       "\n",
       "\n",
       "        [[[ 0.9595,  0.7999,  0.7467,  ..., -2.3915, -2.3915, -2.4092],\n",
       "          [ 0.9063,  0.7822,  0.7290,  ..., -2.3738, -2.3738, -2.3738],\n",
       "          [ 0.8886,  0.7999,  0.7113,  ..., -2.4092, -2.4092, -2.4092],\n",
       "          ...,\n",
       "          [-0.9731, -1.1681, -1.2745,  ..., -1.9837, -1.9837, -1.9837],\n",
       "          [-1.2922, -1.4163, -0.8312,  ..., -1.9837, -1.9837, -1.9660],\n",
       "          [-1.1149, -0.7958, -1.0263,  ..., -1.9837, -1.9660, -1.9482]],\n",
       "\n",
       "         [[ 0.6908,  0.4632,  0.3494,  ..., -2.0024, -2.0024, -2.0214],\n",
       "          [ 0.6908,  0.4822,  0.3684,  ..., -1.9645, -1.9645, -1.9645],\n",
       "          [ 0.7098,  0.5391,  0.3684,  ..., -1.9645, -1.9645, -1.9645],\n",
       "          ...,\n",
       "          [-1.0920, -1.3196, -1.4334,  ..., -1.6800, -1.6800, -1.6800],\n",
       "          [-1.5472, -1.6800, -1.0541,  ..., -1.6800, -1.6800, -1.6610],\n",
       "          [-1.4144, -1.0730, -1.3196,  ..., -1.6800, -1.6610, -1.6420]],\n",
       "\n",
       "         [[-0.4598, -0.7644, -0.9472,  ..., -1.7190, -1.7190, -1.7394],\n",
       "          [-0.4801, -0.7441, -0.9472,  ..., -1.7190, -1.7190, -1.7190],\n",
       "          [-0.4801, -0.7035, -0.9472,  ..., -1.7190, -1.7190, -1.7190],\n",
       "          ...,\n",
       "          [-1.2113, -1.4550, -1.5972,  ..., -1.4956, -1.4956, -1.4956],\n",
       "          [-1.6175, -1.8003, -1.1300,  ..., -1.4956, -1.4956, -1.4753],\n",
       "          [-1.4550, -1.0894, -1.3941,  ..., -1.4956, -1.4753, -1.4550]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5978,  1.5978,  1.5978,  ...,  1.1723,  1.1900,  1.1900],\n",
       "          [ 1.5978,  1.5978,  1.5978,  ...,  1.1723,  1.1900,  1.1900],\n",
       "          [ 1.5978,  1.5978,  1.5978,  ...,  1.1723,  1.1900,  1.1900],\n",
       "          ...,\n",
       "          [ 1.1723,  1.1545,  1.1368,  ...,  0.6936,  0.7467,  0.7999],\n",
       "          [ 1.1723,  1.1545,  1.1368,  ...,  0.6758,  0.7467,  0.7822],\n",
       "          [ 1.1723,  1.1545,  1.1368,  ...,  0.6758,  0.7467,  0.7822]],\n",
       "\n",
       "         [[ 1.5253,  1.5253,  1.5253,  ...,  1.1081,  1.1460,  1.1460],\n",
       "          [ 1.5253,  1.5253,  1.5253,  ...,  1.1081,  1.1460,  1.1460],\n",
       "          [ 1.5253,  1.5253,  1.5253,  ...,  1.1081,  1.1460,  1.1460],\n",
       "          ...,\n",
       "          [ 0.2546,  0.2356,  0.2167,  ..., -0.2765, -0.2196, -0.1627],\n",
       "          [ 0.2546,  0.2356,  0.2167,  ..., -0.2954, -0.2196, -0.1816],\n",
       "          [ 0.2546,  0.2356,  0.2167,  ..., -0.2954, -0.2196, -0.1816]],\n",
       "\n",
       "         [[ 0.9417,  0.9417,  0.9417,  ...,  0.6979,  0.7182,  0.7182],\n",
       "          [ 0.9417,  0.9417,  0.9417,  ...,  0.6979,  0.7182,  0.7182],\n",
       "          [ 0.9417,  0.9417,  0.9417,  ...,  0.6979,  0.7182,  0.7182],\n",
       "          ...,\n",
       "          [-0.2364, -0.2567, -0.2770,  ..., -0.5410, -0.4598, -0.3785],\n",
       "          [-0.2364, -0.2567, -0.2770,  ..., -0.5410, -0.4395, -0.3988],\n",
       "          [-0.2364, -0.2567, -0.2770,  ..., -0.5207, -0.4395, -0.3785]]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d39bb",
   "metadata": {},
   "source": [
    "Here, we normalize just a single batch of images because we do not\n",
    "know yet how to operate on an entire dataset. In working with images, it is good\n",
    "practice to compute the mean and standard deviation on all the training data\n",
    "in advance and then subtract and divide by these fixed, precomputed quantities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939af06",
   "metadata": {},
   "source": [
    "## 3D Images - Volumetric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca4e47",
   "metadata": {},
   "source": [
    "In some contexts, such as medical imaging applications involving, say, CT (computed\n",
    "tomography) scans, we typically deal with sequences of images stacked along the headto-foot axis, each corresponding to a slice across the human body. In CT scans, the intensity represents the density of the different parts of the body—lungs, fat, water, muscle,\n",
    "and bone, in order of increasing density—mapped from dark to bright when the CT\n",
    "scan is displayed on a clinical workstation. The density at each point is computed from\n",
    "the amount of X-rays reaching a detector after crossing through the body, with some\n",
    "complex math to deconvolve the raw sensor data into the full volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f613b",
   "metadata": {},
   "source": [
    "CTs have only a single intensity channel, similar to a grayscale image. This means\n",
    "that often, the channel dimension is left out in native data formats. By stacking individual 2D\n",
    "slices into a 3D tensor, we can build volumetric data representing the 3D anatomy of a\n",
    "subject.\n",
    "For now, it suffices to say that there’s no fundamental difference between a tensor storing volumetric data versus image data. We just have an extra dimension, depth, after the channel\n",
    "dimension, leading to a 5D tensor of shape N × C × D × H × W\n",
    "\n",
    "Let’s load a sample CT scan using the volread function in the imageio module, which\n",
    "takes a directory as an argument and assembles all Digital Imaging and Communications in Medicine (DICOM) files2 in a series in a NumPy 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53aab99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DICOM (examining files): 1/99 files (1.0%15/99 files (15.2%30/99 files (30.3%45/99 files (45.5%61/99 files (61.6%78/99 files (78.8%95/99 files (96.0%99/99 files (100.0%)\n",
      "  Found 1 correct series.\n",
      "Reading DICOM (loading data): 24/99  (24.245/99  (45.566/99  (66.791/99  (91.999/99  (100.0%)\n"
     ]
    }
   ],
   "source": [
    "dir_path = './dlwpt-code-master/dlwpt-code-master/data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083/'\n",
    "vol_read = imageio.volread(dir_path, 'DICOM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a73e7dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vol_read[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144b1be",
   "metadata": {},
   "source": [
    "As was true in section 4.1.3, the layout is different from what PyTorch expects, due to\n",
    "having no channel information. So we’ll have to make room for the channel dimension using unsqueeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a22f4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 99, 512, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol = torch.from_numpy(vol_read).float()\n",
    "vol = torch.unsqueeze(vol, axis=0)\n",
    "vol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4d826",
   "metadata": {},
   "source": [
    "At this point we could assemble a 5D dataset by stacking multiple volumes along the\n",
    "batch direction, just as we did in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "317c7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 3\n",
    "# batch = torch.zeros(batch_size, 1, 99, vol.shape[2], vol.shape[3]).to(torch.uint8)\n",
    "# batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95d4f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = './dlwpt-code-master/dlwpt-code-master/data/p1ch4/volumetric-dicom/2-LUNG 3.0  B70f-04083/'\n",
    "# filenames = [name for name in os.listdir(data_dir) if os.path.splitext(name)[-1] == '.dcm']\n",
    "# filenames[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9a415615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, file in enumerate(filenames):\n",
    "#     img_arr = imageio.volread(os.path.join(data_dir, file))\n",
    "#     img_t = torch.from_numpy(img_arr).float()\n",
    "#     img_t = torch.unsqueeze(img_t, axis=0)\n",
    "#     batch[i] = img_t\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5520f446",
   "metadata": {},
   "source": [
    "## Representing tabular data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb7edd",
   "metadata": {},
   "source": [
    "The simplest form of data we’ll encounter on a machine learning job is sitting in a\n",
    "spreadsheet, CSV file, or database. Whatever the medium, it’s a table containing one\n",
    "row per sample (or record), where columns contain one piece of information about\n",
    "our sample.\n",
    "\n",
    "Columns may contain numerical values, like temperatures at specific locations; or\n",
    "labels, like a string expressing an attribute of the sample, like “blue.” Therefore, tabular data is typically not homogeneous: different columns don’t have the same type. We\n",
    "might have a column showing the weight of apples and another encoding their color\n",
    "in a label. PyTorch tensors, on the other hand, are homogeneous. Information in PyTorch is\n",
    "typically encoded as a number, typically floating-point (though integer types and\n",
    "Boolean are supported as well). This numeric encoding is deliberate, since neural\n",
    "networks are mathematical entities that take real numbers as inputs and produce real\n",
    "numbers as output through successive application of matrix multiplications and\n",
    "nonlinear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a2f87",
   "metadata": {},
   "source": [
    "### Using a real-world dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02fa5e",
   "metadata": {},
   "source": [
    "Our first job as deep learning practitioners is to encode heterogeneous, real-world\n",
    "data into a tensor of floating-point numbers, ready for consumption by a neural network. The Wine Quality dataset is a freely available table containing\n",
    "chemical characterizations of samples of vinho verde, a wine from north Portugal,\n",
    "together with a sensory quality score.\n",
    "\n",
    "The file contains a comma-separated collection of values organized in 12 columns\n",
    "preceded by a header line containing the column names. The first 11 columns contain values of chemical variables, and the last column contains the sensory quality\n",
    "score from 0 (very bad) to 10 (excellent).\n",
    "\n",
    "A possible machine learning task on this dataset is predicting the quality score from\n",
    "chemical characterization alone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34da7f6c",
   "metadata": {},
   "source": [
    "### Loading a wine data tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90b877",
   "metadata": {},
   "source": [
    "Before we can get to that, however, we need to be able to examine the data in a more\n",
    "usable way than opening the file in a text editor. Let’s see how we can load the data\n",
    "using Python and then turn it into a PyTorch tensor. Python offers several options for\n",
    "quickly loading a CSV file. Three popular options are\n",
    "- The csv module that ships with Python\n",
    "- NumPy\n",
    "- Pandas\n",
    "\n",
    "The third option is the most time- and memory-efficient. However, we’ll avoid introducing an additional library in our learning trajectory just because we need to load a\n",
    "file. Since we already introduced NumPy in the previous section, and PyTorch has\n",
    "excellent NumPy interoperability, we’ll go with that. Let’s load our file and turn the\n",
    "resulting NumPy array into a PyTorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ae901aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('./dlwpt-code-master/dlwpt-code-master/data/p1ch4/tabular-wine/winequality-white.csv', delimiter=';')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0455d813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_path = './winequality-white.csv'\n",
    "wineq_numpy = np.loadtxt(wine_path, dtype=np.float32, delimiter=\";\", skiprows=1)\n",
    "wineq_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020961f",
   "metadata": {},
   "source": [
    "Here the data can be seen as 2D array (32-bit floating-point),\n",
    "the delimiter used to separate values in each row, and the fact that the first line should\n",
    "not be read since it contains the column names. Let’s check that all the data has been\n",
    "read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b5574207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fixed acidity',\n",
       " 'volatile acidity',\n",
       " 'citric acid',\n",
       " 'residual sugar',\n",
       " 'chlorides',\n",
       " 'free sulfur dioxide',\n",
       " 'total sulfur dioxide',\n",
       " 'density',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol',\n",
       " 'quality']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_list = next(csv.reader(open(wine_path), delimiter=';'))\n",
    "col_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8680ab4",
   "metadata": {},
   "source": [
    "convert the NumPy array to a PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3b87e109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wineq = torch.from_numpy(wineq_numpy).float()\n",
    "wineq.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353ab4f",
   "metadata": {},
   "source": [
    "At this point, we have a floating-point torch.Tensor containing all the columns,\n",
    "including the last, which refers to the quality score. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0ce4a",
   "metadata": {},
   "source": [
    "### Representing scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475f2e8",
   "metadata": {},
   "source": [
    "We could treat the score as a continuous variable, keep it as a real number, and perform a regression task, or treat it as a label and try to guess the label from the chemical analysis in a classification task. In both approaches, we will typically remove the\n",
    "score from the tensor of input data and keep it in a separate tensor, so that we can use\n",
    "the score as the ground truth without it being input to our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "42ead5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898, 11])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = wineq[:, :-1]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c57200ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4898])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = wineq[:, -1].long()\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e588bf7",
   "metadata": {},
   "source": [
    "If we want to transform the target tensor in a tensor of labels, we have two options,\n",
    "depending on the strategy or what we use the categorical data for. One is simply to\n",
    "treat labels as an integer vector of scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "84609701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f85496",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "The other approach is to build a one-hot encoding of the scores: that is, encode each of\n",
    "the 10 scores in a vector of 10 elements, with all elements set to 0 but one, at a different index for each score. This way, a score of 1 could be mapped onto the vector\n",
    "(1,0,0,0,0,0,0,0,0,0), a score of 5 onto (0,0,0,0,1,0,0,0,0,0), and so on. Note\n",
    "that the fact that the score corresponds to the index of the nonzero element is purely\n",
    "incidental: we could shuffle the assignment, and nothing would change from a classification standpoint.\n",
    "\n",
    "There’s a marked difference between the two approaches. Keeping wine quality\n",
    "scores in an integer vector of scores induces an ordering on the scores—which might\n",
    "be totally appropriate in this case, since a score of 1 is lower than a score of 4. It also\n",
    "induces some sort of distance between scores: that is, the distance between 1 and 3 is the\n",
    "same as the distance between 2 and 4. If this holds for our quantity, then great. If, on\n",
    "the other hand, scores are purely discrete, like grape variety, one-hot encoding will be\n",
    "a much better fit, as there’s no implied ordering or distance. One-hot encoding is also\n",
    "appropriate for quantitative scores when fractional values in between integer scores,\n",
    "like 2.4, make no sense for the application—for when the score is either this or that\n",
    "\n",
    "We can achieve one-hot encoding using the scatter_ method, which fills the tensor with values from a source tensor along the indices provided as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "dde2e9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot = torch.zeros(target.shape[0], 10)\n",
    "target_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc17e43",
   "metadata": {},
   "source": [
    "As scatter expects the index elements to be int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "81cb5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e489b041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot.scatter_(1, target.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704bf7a",
   "metadata": {},
   "source": [
    "As we can see that scatter_ name ends with an underscore. This is a convention in PyTorch that indicates\n",
    "the method will not return a new tensor, but will instead modify the tensor in place.\n",
    "\n",
    "The arguments for scatter_ are as follows:\n",
    "- The dimension along which the following two arguments are specified\n",
    "- A column tensor indicating the indices of the elements to scatter\n",
    "- A tensor containing the elements to scatter or a single scalar to scatter (1, in this case)\n",
    "\n",
    "In other words, the previous invocation reads, “For each row, take the index of the target label (which coincides with the score in our case) and use it as the column index\n",
    "to set the value 1.0.” The end result is a tensor encoding categorical information.\n",
    "The second argument of scatter_, the index tensor, is required to have the same\n",
    "number of dimensions as the tensor we scatter into. Since target_onehot has two\n",
    "dimensions (4,898 × 10), we need to add an extra dummy dimension to target using\n",
    "unsqueeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c04607aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.unsqueeze(1)[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460e4a1",
   "metadata": {},
   "source": [
    "The call to unsqueeze adds a singleton dimension, from a 1D tensor of 4,898 elements\n",
    "to a 2D tensor of size (4,898 × 1), without changing its contents—no extra elements\n",
    "are added; we just decided to use an extra index to access the elements. That is, we\n",
    "access the first element of target as target[0] and the first element of its\n",
    "unsqueezed counterpart as target_unsqueezed[0,0].\n",
    "PyTorch allows us to use class indices directly as targets while training neural networks. However, if we wanted to use the score as a categorical input to the network, we\n",
    "would have to transform it to a one-hot-encoded tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8cd2f",
   "metadata": {},
   "source": [
    "### When to categorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d3953",
   "metadata": {},
   "source": [
    "Let’s go back to our datatensor, containing the 11 variables associated with the chemical\n",
    "analysis. We can use the functions in the PyTorch Tensor API to manipulate our data in\n",
    "tensor form. Let’s first obtain the mean and standard deviations for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "de1bfd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8548e+00, 2.7824e-01, 3.3419e-01, 6.3914e+00, 4.5772e-02, 3.5308e+01,\n",
       "        1.3836e+02, 9.9403e-01, 3.1883e+00, 4.8985e-01, 1.0514e+01])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mean = torch.mean(data, dim=0)\n",
    "data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "75e9975e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.1211e-01, 1.0160e-02, 1.4646e-02, 2.5726e+01, 4.7733e-04, 2.8924e+02,\n",
       "        1.8061e+03, 8.9455e-06, 2.2801e-02, 1.3025e-02, 1.5144e+00])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_var = torch.var(data, dim=0)\n",
    "data_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c2245",
   "metadata": {},
   "source": [
    "In this case, dim=0 indicates that the reduction is performed along dimension 0. At\n",
    "this point, we can normalize the data by subtracting the mean and dividing by the\n",
    "standard deviation, which helps with the learning process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5e23bdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1721, -0.0818,  0.2133,  2.8211, -0.0354,  0.5699,  0.7445,  2.3313,\n",
       "        -1.2468, -0.3491, -1.3930])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_normalized = (data-data_mean)/torch.sqrt(data_var)\n",
    "data_normalized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7da1bd",
   "metadata": {},
   "source": [
    "### Finding thresholds\n",
    "Next, let’s start to look at the data with an eye to seeing if there is an easy way to tell\n",
    "good and bad wines apart at a glance. First, we’re going to determine which rows in\n",
    "target correspond to a score less than or equal to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f9e5dd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898]), tensor(20))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_indexes = target <= 3\n",
    "bad_indexes.shape, bad_indexes.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c22d3c",
   "metadata": {},
   "source": [
    "Or to directly get those values, we can try following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a408a0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8.5000e+00, 2.6000e-01, 2.1000e-01, 1.6200e+01, 7.4000e-02, 4.1000e+01,\n",
       "          1.9700e+02, 9.9800e-01, 3.0200e+00, 5.0000e-01, 9.8000e+00],\n",
       "         [5.8000e+00, 2.4000e-01, 4.4000e-01, 3.5000e+00, 2.9000e-02, 5.0000e+00,\n",
       "          1.0900e+02, 9.9130e-01, 3.5300e+00, 4.3000e-01, 1.1700e+01],\n",
       "         [9.1000e+00, 5.9000e-01, 3.8000e-01, 1.6000e+00, 6.6000e-02, 3.4000e+01,\n",
       "          1.8200e+02, 9.9680e-01, 3.2300e+00, 3.8000e-01, 8.5000e+00],\n",
       "         [7.1000e+00, 3.2000e-01, 3.2000e-01, 1.1000e+01, 3.8000e-02, 1.6000e+01,\n",
       "          6.6000e+01, 9.9370e-01, 3.2400e+00, 4.0000e-01, 1.1500e+01],\n",
       "         [6.9000e+00, 3.9000e-01, 4.0000e-01, 4.6000e+00, 2.2000e-02, 5.0000e+00,\n",
       "          1.9000e+01, 9.9150e-01, 3.3100e+00, 3.7000e-01, 1.2600e+01],\n",
       "         [1.0300e+01, 1.7000e-01, 4.7000e-01, 1.4000e+00, 3.7000e-02, 5.0000e+00,\n",
       "          3.3000e+01, 9.9390e-01, 2.8900e+00, 2.8000e-01, 9.6000e+00],\n",
       "         [7.9000e+00, 6.4000e-01, 4.6000e-01, 1.0600e+01, 2.4400e-01, 3.3000e+01,\n",
       "          2.2700e+02, 9.9830e-01, 2.8700e+00, 7.4000e-01, 9.1000e+00],\n",
       "         [8.3000e+00, 3.3000e-01, 4.2000e-01, 1.1500e+00, 3.3000e-02, 1.8000e+01,\n",
       "          9.6000e+01, 9.9110e-01, 3.2000e+00, 3.2000e-01, 1.2400e+01],\n",
       "         [8.6000e+00, 5.5000e-01, 3.5000e-01, 1.5550e+01, 5.7000e-02, 3.5500e+01,\n",
       "          3.6650e+02, 1.0001e+00, 3.0400e+00, 6.3000e-01, 1.1000e+01],\n",
       "         [7.5000e+00, 3.2000e-01, 2.4000e-01, 4.6000e+00, 5.3000e-02, 8.0000e+00,\n",
       "          1.3400e+02, 9.9580e-01, 3.1400e+00, 5.0000e-01, 9.1000e+00],\n",
       "         [6.7000e+00, 2.5000e-01, 2.6000e-01, 1.5500e+00, 4.1000e-02, 1.1850e+02,\n",
       "          2.1600e+02, 9.9490e-01, 3.5500e+00, 6.3000e-01, 9.4000e+00],\n",
       "         [7.1000e+00, 4.9000e-01, 2.2000e-01, 2.0000e+00, 4.7000e-02, 1.4650e+02,\n",
       "          3.0750e+02, 9.9240e-01, 3.2400e+00, 3.7000e-01, 1.1000e+01],\n",
       "         [1.1800e+01, 2.3000e-01, 3.8000e-01, 1.1100e+01, 3.4000e-02, 1.5000e+01,\n",
       "          1.2300e+02, 9.9970e-01, 2.9300e+00, 5.5000e-01, 9.7000e+00],\n",
       "         [7.6000e+00, 4.8000e-01, 3.7000e-01, 1.2000e+00, 3.4000e-02, 5.0000e+00,\n",
       "          5.7000e+01, 9.9256e-01, 3.0500e+00, 5.4000e-01, 1.0400e+01],\n",
       "         [6.1000e+00, 2.0000e-01, 3.4000e-01, 9.5000e+00, 4.1000e-02, 3.8000e+01,\n",
       "          2.0100e+02, 9.9500e-01, 3.1400e+00, 4.4000e-01, 1.0100e+01],\n",
       "         [4.2000e+00, 2.1500e-01, 2.3000e-01, 5.1000e+00, 4.1000e-02, 6.4000e+01,\n",
       "          1.5700e+02, 9.9688e-01, 3.4200e+00, 4.4000e-01, 8.0000e+00],\n",
       "         [9.4000e+00, 2.4000e-01, 2.9000e-01, 8.5000e+00, 3.7000e-02, 1.2400e+02,\n",
       "          2.0800e+02, 9.9395e-01, 2.9000e+00, 3.8000e-01, 1.1000e+01],\n",
       "         [6.2000e+00, 2.3000e-01, 3.5000e-01, 7.0000e-01, 5.1000e-02, 2.4000e+01,\n",
       "          1.1100e+02, 9.9160e-01, 3.3700e+00, 4.3000e-01, 1.1000e+01],\n",
       "         [6.8000e+00, 2.6000e-01, 3.4000e-01, 1.5100e+01, 6.0000e-02, 4.2000e+01,\n",
       "          1.6200e+02, 9.9705e-01, 3.2400e+00, 5.2000e-01, 1.0500e+01],\n",
       "         [6.1000e+00, 2.6000e-01, 2.5000e-01, 2.9000e+00, 4.7000e-02, 2.8900e+02,\n",
       "          4.4000e+02, 9.9314e-01, 3.4400e+00, 6.4000e-01, 1.0500e+01]]),\n",
       " 20)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = data[target <= 3]\n",
    "indexes, len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d3fe2db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cf699",
   "metadata": {},
   "source": [
    "Note that only 20 of the bad_indexes entries are set to True! By using a feature in\n",
    "PyTorch called advanced indexing, we can use a tensor with data type torch.bool to\n",
    "index the data tensor. This will essentially filter data to be only items (or rows) corresponding to True in the indexing tensor. The bad_indexes tensor has the same shape\n",
    "as target, with values of False or True depending on the outcome of the comparison\n",
    "between our threshold and each element in the original target tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fe7ca8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[8.5000e+00, 2.6000e-01, 2.1000e-01, 1.6200e+01, 7.4000e-02, 4.1000e+01,\n",
       "          1.9700e+02, 9.9800e-01, 3.0200e+00, 5.0000e-01, 9.8000e+00],\n",
       "         [5.8000e+00, 2.4000e-01, 4.4000e-01, 3.5000e+00, 2.9000e-02, 5.0000e+00,\n",
       "          1.0900e+02, 9.9130e-01, 3.5300e+00, 4.3000e-01, 1.1700e+01],\n",
       "         [9.1000e+00, 5.9000e-01, 3.8000e-01, 1.6000e+00, 6.6000e-02, 3.4000e+01,\n",
       "          1.8200e+02, 9.9680e-01, 3.2300e+00, 3.8000e-01, 8.5000e+00],\n",
       "         [7.1000e+00, 3.2000e-01, 3.2000e-01, 1.1000e+01, 3.8000e-02, 1.6000e+01,\n",
       "          6.6000e+01, 9.9370e-01, 3.2400e+00, 4.0000e-01, 1.1500e+01],\n",
       "         [6.9000e+00, 3.9000e-01, 4.0000e-01, 4.6000e+00, 2.2000e-02, 5.0000e+00,\n",
       "          1.9000e+01, 9.9150e-01, 3.3100e+00, 3.7000e-01, 1.2600e+01],\n",
       "         [1.0300e+01, 1.7000e-01, 4.7000e-01, 1.4000e+00, 3.7000e-02, 5.0000e+00,\n",
       "          3.3000e+01, 9.9390e-01, 2.8900e+00, 2.8000e-01, 9.6000e+00],\n",
       "         [7.9000e+00, 6.4000e-01, 4.6000e-01, 1.0600e+01, 2.4400e-01, 3.3000e+01,\n",
       "          2.2700e+02, 9.9830e-01, 2.8700e+00, 7.4000e-01, 9.1000e+00],\n",
       "         [8.3000e+00, 3.3000e-01, 4.2000e-01, 1.1500e+00, 3.3000e-02, 1.8000e+01,\n",
       "          9.6000e+01, 9.9110e-01, 3.2000e+00, 3.2000e-01, 1.2400e+01],\n",
       "         [8.6000e+00, 5.5000e-01, 3.5000e-01, 1.5550e+01, 5.7000e-02, 3.5500e+01,\n",
       "          3.6650e+02, 1.0001e+00, 3.0400e+00, 6.3000e-01, 1.1000e+01],\n",
       "         [7.5000e+00, 3.2000e-01, 2.4000e-01, 4.6000e+00, 5.3000e-02, 8.0000e+00,\n",
       "          1.3400e+02, 9.9580e-01, 3.1400e+00, 5.0000e-01, 9.1000e+00],\n",
       "         [6.7000e+00, 2.5000e-01, 2.6000e-01, 1.5500e+00, 4.1000e-02, 1.1850e+02,\n",
       "          2.1600e+02, 9.9490e-01, 3.5500e+00, 6.3000e-01, 9.4000e+00],\n",
       "         [7.1000e+00, 4.9000e-01, 2.2000e-01, 2.0000e+00, 4.7000e-02, 1.4650e+02,\n",
       "          3.0750e+02, 9.9240e-01, 3.2400e+00, 3.7000e-01, 1.1000e+01],\n",
       "         [1.1800e+01, 2.3000e-01, 3.8000e-01, 1.1100e+01, 3.4000e-02, 1.5000e+01,\n",
       "          1.2300e+02, 9.9970e-01, 2.9300e+00, 5.5000e-01, 9.7000e+00],\n",
       "         [7.6000e+00, 4.8000e-01, 3.7000e-01, 1.2000e+00, 3.4000e-02, 5.0000e+00,\n",
       "          5.7000e+01, 9.9256e-01, 3.0500e+00, 5.4000e-01, 1.0400e+01],\n",
       "         [6.1000e+00, 2.0000e-01, 3.4000e-01, 9.5000e+00, 4.1000e-02, 3.8000e+01,\n",
       "          2.0100e+02, 9.9500e-01, 3.1400e+00, 4.4000e-01, 1.0100e+01],\n",
       "         [4.2000e+00, 2.1500e-01, 2.3000e-01, 5.1000e+00, 4.1000e-02, 6.4000e+01,\n",
       "          1.5700e+02, 9.9688e-01, 3.4200e+00, 4.4000e-01, 8.0000e+00],\n",
       "         [9.4000e+00, 2.4000e-01, 2.9000e-01, 8.5000e+00, 3.7000e-02, 1.2400e+02,\n",
       "          2.0800e+02, 9.9395e-01, 2.9000e+00, 3.8000e-01, 1.1000e+01],\n",
       "         [6.2000e+00, 2.3000e-01, 3.5000e-01, 7.0000e-01, 5.1000e-02, 2.4000e+01,\n",
       "          1.1100e+02, 9.9160e-01, 3.3700e+00, 4.3000e-01, 1.1000e+01],\n",
       "         [6.8000e+00, 2.6000e-01, 3.4000e-01, 1.5100e+01, 6.0000e-02, 4.2000e+01,\n",
       "          1.6200e+02, 9.9705e-01, 3.2400e+00, 5.2000e-01, 1.0500e+01],\n",
       "         [6.1000e+00, 2.6000e-01, 2.5000e-01, 2.9000e+00, 4.7000e-02, 2.8900e+02,\n",
       "          4.4000e+02, 9.9314e-01, 3.4400e+00, 6.4000e-01, 1.0500e+01]]),\n",
       " torch.Size([20, 11]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data = data[bad_indexes]\n",
    "bad_data, bad_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed723502",
   "metadata": {},
   "source": [
    "Note that the new bad_data tensor has 20 rows, the same as the number of rows with\n",
    "True in the bad_indexes tensor. It retains all 11 columns. Now we can start to get\n",
    "information about wines grouped into good, middling, and bad categories. Let’s take\n",
    "the .mean() of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6c15efee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 11])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_data = data[target <= 3]\n",
    "mid_data = data[(target > 3) & (target<7)]\n",
    "good_data = data[(target >= 7)]\n",
    "bad_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "05f5b6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.6000e+00, 3.3325e-01, 3.3600e-01, 6.3925e+00, 5.4300e-02, 5.3325e+01,\n",
       "        1.7060e+02, 9.9488e-01, 3.1875e+00, 4.7450e-01, 1.0345e+01])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_mean = torch.mean(bad_data, dim=0)\n",
    "bad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "43d36d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.8869e+00, 2.8153e-01, 3.3644e-01, 6.7051e+00, 4.7841e-02, 3.5424e+01,\n",
       "        1.4183e+02, 9.9447e-01, 3.1808e+00, 4.8707e-01, 1.0265e+01])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_mean = torch.mean(mid_data, dim=0)\n",
    "mid_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8636e8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.7251e+00, 2.6535e-01, 3.2606e-01, 5.2615e+00, 3.8160e-02, 3.4550e+01,\n",
       "        1.2525e+02, 9.9241e-01, 3.2151e+00, 5.0014e-01, 1.1416e+01])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_mean = torch.mean(good_data, dim=0)\n",
    "good_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4ac4d928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fixed acidity', tensor(7.6000), tensor(6.8869), tensor(6.7251)),\n",
       " ('volatile acidity', tensor(0.3332), tensor(0.2815), tensor(0.2653)),\n",
       " ('citric acid', tensor(0.3360), tensor(0.3364), tensor(0.3261)),\n",
       " ('residual sugar', tensor(6.3925), tensor(6.7051), tensor(5.2615)),\n",
       " ('chlorides', tensor(0.0543), tensor(0.0478), tensor(0.0382)),\n",
       " ('free sulfur dioxide', tensor(53.3250), tensor(35.4240), tensor(34.5505)),\n",
       " ('total sulfur dioxide',\n",
       "  tensor(170.6000),\n",
       "  tensor(141.8330),\n",
       "  tensor(125.2453)),\n",
       " ('density', tensor(0.9949), tensor(0.9945), tensor(0.9924)),\n",
       " ('pH', tensor(3.1875), tensor(3.1808), tensor(3.2151)),\n",
       " ('sulphates', tensor(0.4745), tensor(0.4871), tensor(0.5001)),\n",
       " ('alcohol', tensor(10.3450), tensor(10.2648), tensor(11.4160))]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(col_list, bad_mean, mid_mean, good_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "5890af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, fixed acidity       ,   7.60,   6.89   6.73\n",
      " 1, volatile acidity    ,   0.33,   0.28   0.27\n",
      " 2, citric acid         ,   0.34,   0.34   0.33\n",
      " 3, residual sugar      ,   6.39,   6.71   5.26\n",
      " 4, chlorides           ,   0.05,   0.05   0.04\n",
      " 5, free sulfur dioxide ,  53.33,  35.42  34.55\n",
      " 6, total sulfur dioxide, 170.60, 141.83 125.25\n",
      " 7, density             ,   0.99,   0.99   0.99\n",
      " 8, pH                  ,   3.19,   3.18   3.22\n",
      " 9, sulphates           ,   0.47,   0.49   0.50\n",
      "10, alcohol             ,  10.34,  10.26  11.42\n"
     ]
    }
   ],
   "source": [
    "for i, arg in enumerate(zip(col_list, bad_mean, mid_mean, good_mean)):\n",
    "    print('{:2}, {:20}, {:6.2f}, {:6.2f} {:6.2f}'.format(i, *arg))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66737c",
   "metadata": {},
   "source": [
    "It looks like we’re on to something here: at first glance, the bad wines seem to have\n",
    "higher total sulfur dioxide, among other differences. We could use a threshold on\n",
    "total sulfur dioxide as a crude criterion for discriminating good wines from bad ones.\n",
    "Let’s get the indexes where the total sulfur dioxide column is below the midpoint we\n",
    "calculated earlier, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "164ca950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([170., 132.,  97.,  ..., 111., 110.,  98.])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sulfer_threshold = 141.83\n",
    "total_sulfer_data = data[:, 6]\n",
    "total_sulfer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "45674f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2727)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_index = torch.lt(total_sulfer_data, total_sulfer_threshold)\n",
    "predicted_index.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff222bc",
   "metadata": {},
   "source": [
    "This means our threshold implies that just over half of all the wines are going to be\n",
    "high quality. Next, we’ll need to get the indexes of the actually good wines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "3b2cf4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3258)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_index = target > 5\n",
    "actual_index.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3567099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_matches = torch.sum(actual_index & predicted_index ).item()\n",
    "n_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "3104c04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2727"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_predicted = torch.sum(predicted_index).item()\n",
    "n_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2e2597b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3258"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actual = torch.sum(actual_index).item()\n",
    "n_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "69e062aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2018, 0.74000733406674, 0.6193984039287906)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_matches, n_matches/n_predicted, n_matches/n_actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde7b2c",
   "metadata": {},
   "source": [
    "We got around 2,000 wines right! Since we predicted 2,700 wines, this gives us a 74%\n",
    "chance that if we predict a wine to be high quality, it actually is. Unfortunately, there\n",
    "are 3,200 good wines, and we only identified 61% of them. Well, we got what we\n",
    "signed up for; that’s barely better than random! Of course, this is all very naive: we\n",
    "know for sure that multiple variables contribute to wine quality, and the relationships\n",
    "between the values of these variables and the outcome (which could be the actual\n",
    "score, rather than a binarized version of it) is likely more complicated than a simple\n",
    "threshold on a single value.\n",
    "\n",
    "Indeed, a simple neural network would overcome all of these limitations, as would\n",
    "a lot of other basic machine learning methods. We’ll have the tools to tackle this problem after the next two chapters, once we have learned how to build our first neuralWorking with network from scratch. We will also revisit how to better grade our results in chapter 12.\n",
    "Let’s move on to other data types for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f70fc08",
   "metadata": {},
   "source": [
    "### Working with time series\n",
    "Going back to the wine dataset, we could have had a “year” column that allowed us\n",
    "to look at how wine quality evolved year after year. Unfortunately, we don’t have such\n",
    "data at hand, but we’re working hard on manually collecting the data samples, bottle\n",
    "by bottle. (Stuff for our second edition.) In the meantime, we’ll switch to another\n",
    "interesting dataset: data from a Washington, D.C., bike-sharing system reporting the\n",
    "hourly count of rental bikes in 2011–2012 in the Capital Bikeshare system, along with\n",
    "weather and seasonal information (available here: http://mng.bz/jgOx). Our goal\n",
    "will be to take a flat, 2D dataset and transform it into a 3D one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bab90c",
   "metadata": {},
   "source": [
    "### Adding a time dimension\n",
    "In the source data, each row is a separate hour of data (figure 4.5 shows a transposed\n",
    "version of this to better fit on the printed page). We want to change the row-per-hour\n",
    "organization so that we have one axis that increases at a rate of one day per index increment, and another axis that represents the hour of the day (independent of the date).\n",
    "The third axis will be our different columns of data (weather, temperature, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ac9a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 3.0000e+00, 1.3000e+01,\n",
       "         1.6000e+01],\n",
       "        [2.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 8.0000e+00, 3.2000e+01,\n",
       "         4.0000e+01],\n",
       "        [3.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 5.0000e+00, 2.7000e+01,\n",
       "         3.2000e+01],\n",
       "        ...,\n",
       "        [1.7377e+04, 3.1000e+01, 1.0000e+00,  ..., 7.0000e+00, 8.3000e+01,\n",
       "         9.0000e+01],\n",
       "        [1.7378e+04, 3.1000e+01, 1.0000e+00,  ..., 1.3000e+01, 4.8000e+01,\n",
       "         6.1000e+01],\n",
       "        [1.7379e+04, 3.1000e+01, 1.0000e+00,  ..., 1.2000e+01, 3.7000e+01,\n",
       "         4.9000e+01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes_numpy = np.loadtxt('./dlwpt-code-master/bike-sharing-dataset/hour-fixed.csv', dtype=np.float32, delimiter=',',skiprows=1,\n",
    "          converters={1: lambda x: float(x[8:10])})\n",
    "bikes = torch.from_numpy(bikes_numpy)\n",
    "bikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "8f660edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = pd.read_csv('./dlwpt-code-master/bike-sharing-dataset/hour-fixed.csv', delimiter=',')\n",
    "# dat[\"dteday\"][17375], float(dat[\"dteday\"][17375][8:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9236a",
   "metadata": {},
   "source": [
    "For every hour, the dataset reports the following variables:\n",
    "- Index of record: instant\n",
    "- Day of month: day\n",
    "- Season: season (1: spring, 2: summer, 3: fall, 4: winter)\n",
    "- Year: yr (0: 2011, 1: 2012)\n",
    "- Month: mnth (1 to 12)\n",
    "- Hour: hr (0 to 23)\n",
    "- Holiday status: holiday\n",
    "- Day of the week: weekday\n",
    "- Working day status: workingday\n",
    "- Weather situation: weathersit (1: clear, 2:mist, 3: light rain/snow, 4: heavy\n",
    "rain/snow)\n",
    "- Temperature in °C: temp\n",
    "- Perceived temperature in °C: atemp\n",
    "- Humidity: hum\n",
    "- Wind speed: windspeed\n",
    "- Number of casual users: casual\n",
    "- Number of registered users: registered\n",
    "- Count of rental bikes: cnt\n",
    "\n",
    "In a time series dataset such as this one, rows represent successive time-points: there is\n",
    "a dimension along which they are ordered. Sure, we could treat each row as independent and try to predict the number of circulating bikes based on, say, a particular time\n",
    "of day regardless of what happened earlier. However, the existence of an ordering\n",
    "gives us the opportunity to exploit causal relationships across time. For instance, it\n",
    "allows us to predict bike rides at one time based on the fact that it was raining at an\n",
    "earlier time. For the time being, we’re going to focus on learning how to turn our\n",
    "bike-sharing dataset into something that our neural network will be able to ingest in\n",
    "fixed-size chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418381d",
   "metadata": {},
   "source": [
    "### Shaping the data by time period\n",
    "We might want to break up the two-year dataset into wider observation periods, like\n",
    "days. This way we’ll have N (for number of samples) collections of C sequences of length\n",
    "L. In other words, our time series dataset would be a tensor of dimension 3 and shape\n",
    "N × C × L. The C would remain our 17 channels, while L would be 24: 1 per hour of\n",
    "the day. There’s no particular reason why we must use chunks of 24 hours, though the\n",
    "general daily rhythm is likely to give us patterns we can exploit for predictions. We\n",
    "could also use 7 × 24 = 168 hour blocks to chunk by week instead, if we desired. All of\n",
    "this depends, naturally, on our dataset having the right size—the number of rows must\n",
    "be a multiple of 24 or 168. Also, for this to make sense, we cannot have gaps in the\n",
    "time series.\n",
    "\n",
    "Let’s go back to our bike-sharing dataset. The first column is the index (the global\n",
    "ordering of the data), the second is the date, and the sixth is the time of day. We have\n",
    "everything we need to create a dataset of daily sequences of ride counts and other\n",
    "exogenous variables. Our dataset is already sorted, but if it were not, we could use\n",
    "torch.sort on it to order it appropriately\n",
    "\n",
    "All we have to do to obtain our daily hours dataset is view the same tensor in batches\n",
    "of 24 hours. Let’s take a look at the shape and strides of our bikes tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a425f2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17520, 17]), (17, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikes.shape, bikes.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34ec452d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 24, 17]), (408, 17, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = bikes.view(-1, 24, bikes.shape[1])\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81fb73",
   "metadata": {},
   "source": [
    "What happened here? First, bikes.shape[1] is 17, the number of columns in the\n",
    "bikes tensor. But the real crux of this code is the call to view, which is really important: it changes the way the tensor looks at the same data as contained in storage.\n",
    "\n",
    "Calling view on a tensor returns a new tensor that changes the number of dimensions and the striding information, without\n",
    "changing the storage. This means we can rearrange our tensor at basically zero cost,\n",
    "because no data will be copied. Our call to view requires us to provide the new shape\n",
    "for the returned tensor. We use -1 as a placeholder for “however many indexes are\n",
    "left, given the other dimensions and the original number of elements.\n",
    "\n",
    "Storage is a contiguous, linear container for numbers (floating-point, in this case). Our bikes tensor will have each row\n",
    "stored one after the other in its corresponding storage. This is confirmed by the output from the call to bikes.stride() earlier.\n",
    "For daily_bikes, the stride is telling us that advancing by 1 along the hour dimension (the second dimension) requires us to advance by 17 places in the storage (or\n",
    "one set of columns); whereas advancing along the day dimension (the first dimension) requires us to advance by a number of elements equal to the length of a row in\n",
    "the storage times 24 (here, 408, which is 17 × 24).\n",
    "\n",
    "We see that the rightmost dimension is the number of columns in the original\n",
    "dataset. Then, in the middle dimension, we have time, split into chunks of 24 sequential hours. In other words, we now have N sequences of L hours in a day, for C channels. To get to our desired N × C × L ordering, we need to transpose the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "563cbb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([730, 17, 24]), (408, 1, 17))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = daily_bikes.transpose(1, 2)\n",
    "daily_bikes.shape, daily_bikes.stride()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3390b8d",
   "metadata": {},
   "source": [
    "### Ready for Training\n",
    "The “weather situation” variable is ordinal. It has four levels: 1 for good weather, and 4\n",
    "for, er, really bad. We could treat this variable as categorical, with levels interpreted as\n",
    "labels, or as a continuous variable. If we decided to go with categorical, we would turn\n",
    "the variable into a one-hot-encoded vector and concatenate the columns with the\n",
    "dataset.\n",
    "\n",
    "In order to make it easier to render our data, we’re going to limit ourselves to the\n",
    "first day for a moment. We initialize a zero-filled matrix with a number of rows equal\n",
    "to the number of hours in the day and number of columns equal to the number of\n",
    "weather levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "391e116e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_day = bikes[:24].long()\n",
    "first_day[:, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9305af09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot = torch.zeros(first_day.shape[0], 4)\n",
    "weather_onehot[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8137ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_onehot.scatter_(dim=1, index=first_day[:, 9].unsqueeze(1).long() - 1, value=1.0)\n",
    "weather_onehot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfaf51-d072-46b7-bfa5-3c2e3938996c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "584ca696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.0000,  1.0000,  0.0000,  1.0000,  0.0000,  0.0000,  6.0000,\n",
       "         0.0000,  1.0000,  0.2400,  0.2879,  0.8100,  0.0000,  3.0000, 13.0000,\n",
       "        16.0000,  1.0000,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((bikes[:24], weather_onehot), dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bc6e2-6b14-48e5-b8f4-72b7335ef67f",
   "metadata": {},
   "source": [
    "Here we prescribed our original bikes dataset and our one-hot-encoded “weather situation” matrix to be concatenated along the column dimension (that is, 1). In other\n",
    "words, the columns of the two datasets are stacked together; or, equivalently, the new\n",
    "one-hot-encoded columns are appended to the original dataset. For cat to succeed, it\n",
    "is required that the tensors have the same size along the other dimensions—the row\n",
    "dimension, in this case. Note that our new last four columns are 1, 0, 0, 0, exactly\n",
    "as we would expect with a weather value of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14e7ce54-aeee-4193-9bea-35a07cba45e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(daily_bikes.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e4114b-0b83-47dc-a2a6-b759d6857834",
   "metadata": {},
   "source": [
    "We could have done the same with the reshaped daily_bikes tensor. Remember\n",
    "that it is shaped (B, C, L), where L = 24. We first create the zero tensor, with the same\n",
    "B and L, but with the number of additional columns as C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05161ddb-c5d4-4e05-b1f5-f1ee48e88d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\n",
    "daily_weather_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c6a34-ef2b-4803-81db-f2a9cf2d8d07",
   "metadata": {},
   "source": [
    "Then we scatter the one-hot encoding into the tensor in the C dimension. Since this\n",
    "operation is performed in place, only the content of the tensor will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d9280f7d-4dcf-4673-bd80-e3d6939f670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([730, 4, 24])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_onehot.scatter_(\n",
    "1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\n",
    "daily_weather_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59438d-1770-4831-9f75-a7162faf3698",
   "metadata": {},
   "source": [
    "And we concatenate along the C dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e18ee3e8-a925-4a1e-a5b0-4d2d22f66bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0896, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.2537, 0.2836, 0.2836, 0.2985, 0.2836, 0.2985, 0.2985, 0.2836,\n",
       "        0.2537, 0.2537, 0.2537, 0.1940, 0.2239, 0.2985])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)\n",
    "daily_bikes[0][13]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e0d96-b325-4e59-950f-f610982fdf49",
   "metadata": {},
   "source": [
    "We mentioned earlier that this is not the only way to treat our “weather situation” variable. Indeed, its labels have an ordinal relationship, so we could pretend they are special values of a continuous variable. We could just transform the variable so that it runs\n",
    "from 0.0 to 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05e6435d-3eaa-448e-92e9-aade6bca32f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4993, -0.4993, -0.4989, -0.4989, -0.4989, -0.4989, -0.4989, -0.4989,\n",
       "        -0.4989, -0.4989, -0.4989, -0.4989, -0.4989, -0.4989, -0.4989, -0.4989,\n",
       "        -0.4989, -0.4989, -0.4984, -0.4984, -0.4984, -0.4984, -0.4984, -0.4984])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1)/3\n",
    "daily_bikes[:, 9, :][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3be8c7-7a39-441f-8262-6fe0e7c0bf83",
   "metadata": {},
   "source": [
    "As we mentioned in the previous section, rescaling variables to the [0.0, 1.0] interval\n",
    "or the [-1.0, 1.0] interval is something we’ll want to do for all quantitative variables,\n",
    "like temperature (column 10 in our dataset). We’ll see why later; for now, let’s just say\n",
    "that this is beneficial to the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff178470-6c30-4228-b7fb-7ff8b56b1368",
   "metadata": {},
   "source": [
    "There are multiple possibilities for rescaling variables. We can either map their\n",
    "range to [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "00a7030d-259e-40aa-85ee-c97273b5c5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = daily_bikes[:, 10, :]\n",
    "temp_min = torch.min(temp)\n",
    "temp_max = torch.max(temp)\n",
    "temp_min, temp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "54e5892e-e259-4087-9fab-3acf727db845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2245, 0.2041, 0.2041,  ..., 0.3878, 0.3878, 0.4490],\n",
       "        [0.4490, 0.4286, 0.4082,  ..., 0.2449, 0.2245, 0.2041],\n",
       "        [0.2041, 0.1837, 0.1837,  ..., 0.1633, 0.1224, 0.1633],\n",
       "        ...,\n",
       "        [0.2245, 0.2245, 0.2245,  ..., 0.2653, 0.2449, 0.2449],\n",
       "        [0.2449, 0.2449, 0.2449,  ..., 0.1837, 0.1837, 0.1837],\n",
       "        [0.1633, 0.1633, 0.1429,  ..., 0.2449, 0.2449, 0.2449]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes[:, 10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf1abd20-9818-4fd7-b40c-81ded62d37ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2245, 0.2041, 0.2041,  ..., 0.3878, 0.3878, 0.4490],\n",
       "         [0.2879, 0.2727, 0.2727,  ..., 0.4091, 0.4091, 0.4545],\n",
       "         [0.8100, 0.8000, 0.8000,  ..., 0.8700, 0.9400, 0.8800],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.4490, 0.4286, 0.4082,  ..., 0.2449, 0.2245, 0.2041],\n",
       "         [0.4545, 0.4394, 0.4242,  ..., 0.2273, 0.2121, 0.2273],\n",
       "         [0.8800, 0.9400, 1.0000,  ..., 0.4400, 0.4400, 0.4700],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2041, 0.1837, 0.1837,  ..., 0.1633, 0.1224, 0.1633],\n",
       "         [0.1970, 0.1667, 0.1667,  ..., 0.1970, 0.1515, 0.2121],\n",
       "         [0.4400, 0.4400, 0.4400,  ..., 0.6400, 0.6900, 0.5500],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.2245, 0.2245, 0.2245,  ..., 0.2653, 0.2449, 0.2449],\n",
       "         [0.2424, 0.2424, 0.2424,  ..., 0.2424, 0.2424, 0.2424],\n",
       "         [0.7000, 0.7500, 0.7000,  ..., 0.5600, 0.6000, 0.6000],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2449, 0.2449, 0.2449,  ..., 0.1837, 0.1837, 0.1837],\n",
       "         [0.2576, 0.2273, 0.2424,  ..., 0.2121, 0.1970, 0.1970],\n",
       "         [0.6000, 0.5600, 0.5600,  ..., 0.5100, 0.5500, 0.5100],\n",
       "         ...,\n",
       "         [1.0000, 1.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.1633, 0.1633, 0.1429,  ..., 0.2449, 0.2449, 0.2449],\n",
       "         [0.1818, 0.1818, 0.1667,  ..., 0.2576, 0.2727, 0.2727],\n",
       "         [0.5500, 0.5500, 0.5900,  ..., 0.6000, 0.5600, 0.6500],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min)\n",
    "/ (temp_max - temp_min))\n",
    "daily_bikes[:, 10:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00a423a-ad9b-4166-868a-7c0155dcebf2",
   "metadata": {},
   "source": [
    "or subtract the mean and divide by the standard deviation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1f57ab07-3ecf-4d0f-95e7-5fb97764e1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3213, -1.4248, -1.4248,  ..., -0.4932, -0.4932, -0.1827],\n",
       "        [-0.1827, -0.2862, -0.3897,  ..., -1.2178, -1.3213, -1.4248],\n",
       "        [-1.4248, -1.5284, -1.5284,  ..., -1.6319, -1.8389, -1.6319],\n",
       "        ...,\n",
       "        [-1.3213, -1.3213, -1.3213,  ..., -1.1143, -1.2178, -1.2178],\n",
       "        [-1.2178, -1.2178, -1.2178,  ..., -1.5284, -1.5284, -1.5284],\n",
       "        [-1.6319, -1.6319, -1.7354,  ..., -1.2178, -1.2178, -1.2178]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - torch.mean(temp))\n",
    "/ torch.std(temp))\n",
    "daily_bikes[:, 10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08deb28e-4477-43ad-ac5e-1f8dd7730a28",
   "metadata": {},
   "source": [
    "In the latter case, our variable will have 0 mean and unitary standard deviation. If our\n",
    "variable were drawn from a Gaussian distribution, 68% of the samples would sit in the\n",
    "[-1.0, 1.0] interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c8263-26b4-4a89-8894-59d2cf5e9b6c",
   "metadata": {},
   "source": [
    "Great: we’ve built another nice dataset, and we’ve seen how to deal with time series\n",
    "data. For this tour d’horizon, it’s important only that we got an idea of how a time\n",
    "series is laid out and how we can wrangle the data in a form that a network will digest.\n",
    "Other kinds of data look like a time series, in that there is a strict ordering. Top\n",
    "two on the list? Text and audio. We’ll take a look at text next, and the “Conclusion”\n",
    "section has links to additional examples for audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8a9b0-64c4-405d-8660-9face94b068e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
